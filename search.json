[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Music Memes",
    "section": "",
    "text": "Welcome\nOn these pages you will learn about cultural evolution and music. The overall aim is to attain a basic understanding of formal models in cultural evolution and learn about several recent approaches that apply them to the domain of music.\nWe start with a minimal introduction to the Python programming language that covers the necessary basic skills in order to follow the remainder of the book. Then, we summarize some general ideas about music and cultural evolution.\nSubsequently, we follow the excellent learning path for computational models in cultural evolution provided by the book Individual-based models of cultural evolution: A step-by-step guide using R (Acerbi et al., 2022). These pages comprise a translation of this resource to Python. Finally, we will review and discuss a number of recent publications on music and cultural evolution in the advanced topics section at the end.\n\n\n\n\n\n\nIf you want to refer to this resource, please cite it as appropriately, e.g.:\nMoss, F. C. (2023). Music memes: Understanding processes of music transmission through cultural evolution modeling. https://fabianmoss.github.io/musicmemes\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe material on this page has been adapted and designed for my musicology research seminar “Music Memes: quantitative approaches and theories of cultural transmission of music” at Würzburg University, Germany.\nNote that the materials here are still under development. Please inform me if you notice any errors or other issues.\n\n\n\nAcknowledgements\nI am grateful for encouragement from Alberto Acerbi to continue working on my Python translation of his book and many helpful comments.\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/"
  },
  {
    "objectID": "intro.html#musical-memes",
    "href": "intro.html#musical-memes",
    "title": "1  Introduction",
    "section": "1.1 Musical Memes",
    "text": "1.1 Musical Memes\nThe term ‘meme’ has been adopted by everyday language and is usually associated with a certain image, superimposed with text, that is shared on the internet and has a particular contextual meaning.\n\n\n\nA musical meme. This image itself has been ended up here after being passed along a chain of cultural transmission processes involving several social media: a Google search for “music meme” led to an image result pointing to Classic FM, which, in turn, had taken this image from the instagram page of user @musicmemes.for.supertonicteens.\n\n\nOriginally, however, the definition of ‘meme’ was much broader. Towards the end of The Selfish Gene, Richard Dawkins introduced the concept as follows:\n\nWe need […] a noun that conveys the idea of a unit of cultural transmission, or a unit of imitation. ‘Mimeme’ comes from a suitable Greek root,1 but I want a monosyllable word that sounds a bit like ‘gene’. I hope my classicist friends will forgive me if I abbreviate mimeme to meme. Dawkins (1976, p. 192)\n\nHere, we adopt this original, more extensive concept of memes that, as we will discover, comprises the more or less funny internet pictures as a special case.\nThe field of cultural evolution emerged in the 1980’s (e.g., Boyd & Richerson, 1985; Cavalli-Sforza & Feldman, 1981), and has, in parallel with the advancement of computational facilities, gained momentum. Theories on cultural evolution share many facets with approaches on memetics (Aunger, 2001; Blackmore, 2000; Dawkins, 1976; Howe & Windram, 2011), a field that has also been applied to the case of music (Jan, 2016).\nIn recent years, several approaches have attempted to apply formal models from cultural evolution to the domain of music.\nIn the present context, we first introduce some central ideas of cultural evolution and review a few major publications for the domain of music.\nA few selected important contributions are:\n\n“Cultural Transmission and Evolution: A Quantitative Approach” (Cavalli-Sforza & Feldman, 1981)\n“Culture and the Evolutionary Process” (Boyd & Richerson, 1985)\n“The Memetics of Music” (Jan, 2016)\n“Cultural Evolution and Music” (Youngblood et al., forthcoming)\n“Cultural Evolution of Music” (Savage, 2019)"
  },
  {
    "objectID": "intro.html#conditions-for-an-evolutionary-system",
    "href": "intro.html#conditions-for-an-evolutionary-system",
    "title": "1  Introduction",
    "section": "1.2 Conditions for an evolutionary system",
    "text": "1.2 Conditions for an evolutionary system\n\nvariation\nselection\nretention\n\nSee Blackmore (2000), Chapter 2 “Universal Darwinism” for an excellent introduction."
  },
  {
    "objectID": "intro.html#music-in-human-evolution",
    "href": "intro.html#music-in-human-evolution",
    "title": "1  Introduction",
    "section": "1.3 Music in human evolution",
    "text": "1.3 Music in human evolution\nThis book is about the cultural evolution of music. It has to be mentioned, however, that there is a large body of research on the biological evolution of music. This research asks questions about with which evolutionary advantages music endowed early humans, whether it is something that we share with other animals or whether it makes us unique. Some hold the view that music has no particularly relevant evolutionary function at all (Pinker, 1997), while others see in it a key to our success as a species (Cross, 2016).\nWhatever the true role of music in the evolutionary history of humanity may have been, it is most certainly a fascinating topic to reflect upon. After all, human musical activity with dedicated instruments can be traced back at least 20,000 years, although it seems more than likely that human ‘musicking’ dates back much further, since our own bodies and voices already provided us with excellent musical instruments long before the first instruments have been devised.\nIf you are interested in learning more about biological-evolutionary aspects of music, I highly recommend to read, e.g. Wallin et al. (2001), Morley (2013), Tomlinson (2018), and Honing (2018)."
  },
  {
    "objectID": "intro.html#history-and-cultural-evolution",
    "href": "intro.html#history-and-cultural-evolution",
    "title": "1  Introduction",
    "section": "1.4 History and cultural evolution",
    "text": "1.4 History and cultural evolution\nDocumenting, describing, and interpreting changes in human culture is what historians do. Accordingly, changes in music belong to the field of music history, or historical musicology. However, most historians would probably be hesitant to employ models, or worse: formal models, in order to describe historical processes. The dictum “history doesn’t repeat itself” seems to raise a fundamental argument against such endeavors that aim at explaining cultural or historical changes by means of underlying latent ‘forces’. Modeling, in that view, seems to erroneously assume that history is teleological – directed towards a predetermined goal.\nOn the other hand, it is undeniable that there are many aspects of culture exhibit regularities and ‘progresses’ that are hard to explain if there are no guiding processes. Defining “culture” is, of course, yet another difficult enterprise. Here, I follow more or less the definition of Boyd & Richerson (1985, p. 33): “Culture is information capable of affecting individuals’ phenotypes which they acquire from other conspecifics by teaching or imitation [emphasis mine]”.2 The part important to us here is “by teaching or imitation”, which is meant to imply: not by genetic inheritance. If humans can transmit information by other means than genetic inheritance, and if these transmission processes continue over many generations, then they are worth studying. Rest assured, the assumption of a hidden goal towards which all cultural processes are directed is not needed at all! I hope that you will share this conviction after working through this material."
  },
  {
    "objectID": "intro.html#schema-theory",
    "href": "intro.html#schema-theory",
    "title": "1  Introduction",
    "section": "1.5 Schema theory",
    "text": "1.5 Schema theory\nWhat are the units (the basic memes) in music? Schema theory proposes a set of more or less fixed patterns, ‘schemata’, that underly a large body of music in the Baroque and Classical period (Gjerdingen, 2007).\nAre schemata really memes? I would think not because they can not mutate freely without losing their meaning. I rather think schemata ‘fall out’ of several combinatorial possibilities to traverse the diatonic scale, and compositional mechanisms to elaborate and vary them. But this kind of variation is very different than the one we talk about in this book. Composed variation is very regular, not random, and relying both on the melodic and metric structures in which it unfolds. Those, however, could very well be understood as environments with very strong constraints (being ‘out-of-scale’ or ‘off-beat’)."
  },
  {
    "objectID": "intro.html#basic-cultural-inheritance-mechanisms",
    "href": "intro.html#basic-cultural-inheritance-mechanisms",
    "title": "1  Introduction",
    "section": "1.6 Basic cultural inheritance mechanisms",
    "text": "1.6 Basic cultural inheritance mechanisms\nFollowing this introduction, we introduce some minimal requirements to use Python for this course (Chapter 3).\nSubsequently, we introduce with six central mechanisms for cultural inheritance: unbiased transmission (Chapter 6), unbiased and biased mutation (Chapter 7), directly biased transmission (Chapter 9), frequency-dependent indirectly biased transmission (Chapter 10), and demonstrator-based indirectly biased transmission (Chapter 11). We follow up with a chapter on vertical and horizontal transmission (Section 12.1), and finally introduce the multiple traits model (Chapter 13). The following diagram gives an overview of these processes:\n\n\nAfter having a firm grasp on how these processes can be modeled in Python and how modeling results can be interpreted, we move to more advanced topics, and more specifically into a number of recent exciting results about cultural evolution and music. We conclude our journey (Chapter 19) with a more general discussion of the implications of cultural evolution for how we think about music, on the relevance of modeling in music research and the humanities more generally, as well as on the great potential of this approach for future research.\n\n\n\n\nAunger, R. (2001). Darwinizing Culture: The Status of Memetics as a Science. Oxford University Press, USA. http://gen.lib.rus.ec/book/index.php?md5=7329e2aa9adcddfed967088219426193\n\n\nBlackmore, S. (2000). The Meme Machine. Oxford University Press.\n\n\nBoyd, R., & Richerson, P. J. (1985). Culture and the Evolutionary Process. The University of Chicago Press.\n\n\nCavalli-Sforza, L. L., & Feldman, M. W. (1981). Cultural Transmission and Evolution. Princeton University Press.\n\n\nCross, I. (2016). The nature of music and its evolution. In S. Hallam, I. Cross, & M. Thaut (Eds.), The Oxford Handbook of Music Psychology (2nd ed., pp. 1–20). Oxford University Press. https://doi.org/10.1093/oxfordhb/9780199298457.013.0001\n\n\nDawkins, R. (1976). The Selfish Gene. Oxford University Press.\n\n\nGjerdingen, R. O. (2007). Music in the Galant Style. Oxford University Press.\n\n\nHoning, H. (2018). On the biological basis of musicality. Annals of the New York Academy of Sciences. https://doi.org/10.1111/nyas.13638\n\n\nHowe, C. J., & Windram, H. F. (2011). Phylomemetics beyond the Gene. PLOS Biology, 9(5), e1001069. https://doi.org/10.1371/journal.pbio.1001069\n\n\nJan, S. (2016). The Memetics of Music: A Neo-Darwinian View of Musical Structure and Culture. Routledge.\n\n\nMorley, I. (2013). The Prehistory of Music. Human Evolution, Archaeology, and the Origins of Musicality. Oxford University Press.\n\n\nPinker, S. (1997). How the mind works. Norton.\n\n\nSavage, P. E. (2019). Cultural evolution of music. Palgrave Communications, 5(1), 1–16. https://doi.org/10.1057/s41599-019-0221-1\n\n\nTomlinson, G. (2018). A Million Years of Music. Princeton University Press. https://press.princeton.edu/books/paperback/9781890951528/a-million-years-of-music\n\n\nWallin, N. L., Merker, B., & Brown, S. (Eds.). (2001). The Origins of Music. MIT Press.\n\n\nYoungblood, M., Ozaki, Y., & Savage, P. E. (forthcoming). Cultural evolution and music. In J. Tehrani, J. R. Kendal, & R. L. Kendal (Eds.), Oxford Handbook of Cultural Evolution. Oxford University Press. https://psyarxiv.com/xsb7v"
  },
  {
    "objectID": "style.html#a-model-of-style-in-music",
    "href": "style.html#a-model-of-style-in-music",
    "title": "2  Style",
    "section": "2.1 A model of style in music",
    "text": "2.1 A model of style in music\nA comprehensive model for theoretical treatment of style was proposed by Meyer (1989). In a nutshell, he proposes a model where certain constraints favor differnt kinds of patternings. He organises these constraints hierarchically so that this model is suitable to describe a variety of stylistic phenomena on different levels. This clear separation is more conceptual and practical only for theoretical discussions. In a real scenario, boundaries are often unclear or ambigous.\n\n2.1.1 Constraints\n\nLaws\nRules\nStrategies\n\n\n\n2.1.2 Patterns\n\nDialect\nIdiom\nIntraopus Style (here, we need a concept of “work”, but the extended “meme” concept or “memecomplex might also be useful”)\n\n\n\n\n\nMeyer, L. B. (1989). Style and Music. Theory, History, and Ideology. University of Chicago Press."
  },
  {
    "objectID": "python_primer.html#variables-and-types",
    "href": "python_primer.html#variables-and-types",
    "title": "3  A Python primer",
    "section": "3.1 Variables and types",
    "text": "3.1 Variables and types\nVariable assignment in Python is straight-forward. You choose a name for the variable, and assign a value to it using the = operator, for example:\n\nx = 100\n\nassigns the value 100 to the variable x. If we call the variable now, we can see that it has the value we assigned to it:\n\nx\n\n100\n\n\nOf course, we can also assign things other than numbers, for example:\n\nname = \"Fabian\"\n\nWhat we assigned to the variable name is called a string, it has the value \"Fabian\". Strings are sequences of characters.\n\n\n\n\n\n\nTip\n\n\n\nNote that \"Fabian\" is enclosed by double-quotes. Why is this the case? Why could we not just type name = Fabian?\n\n\nWe can also assign a list of things to a variable:\n\nmylist = [1, 2, 3, \"A\", \"B\", \"C\"]\n\nLists are enclosed by square brackets. As you can see, Python allows you to store any kind of data in lists (here: integer numbers and character strings). However, it is good practice to include only—you’ll understand later why.\nAnother structured data type in python are dictionaries. Dictionaries are collections of key-value pairs. For example, a dictionary addresses could store the email addresses of certain people:\n\naddresses = {\n    \"Andrew\" : \"andrew@example.com\",\n    \"Zoe\" : \"zoe@example.com\"\n}\n\nNow, if we wanted to look up Zoe’s email address, we could to so with:\n\naddresses[\"Zoe\"]\n\n'zoe@example.com'"
  },
  {
    "objectID": "python_primer.html#on-repeat",
    "href": "python_primer.html#on-repeat",
    "title": "3  A Python primer",
    "section": "3.2 On repeat",
    "text": "3.2 On repeat\nCoding something is only useful if you can’t do the job as fast or as efficient by yourself. Especially when it comes to repeating the same little thing many, many times, knowing how to code comes in handy.\nAs a simple example, imagine you want to write down all numbers from 1 to 10, or from 1 to 100, or… you get the idea. In Python, you would do it as follows:\n\nfor i in range(10):\n    print(i, end=\" \")\n\n0 1 2 3 4 5 6 7 8 9 \n\n\nYou see that this is not exactly what we wanted. We’re seeing numbers from 0 to 9, but we wanted everything from 1 to 10. Before we fix the code to produce the desired result, let’s explain the bits and pieces of the code above. What we just did was to use a so-called for-loop, probably the most common way to repeat things in Python. First we create an iterator variable i (we could have named any other variable name as well), which takes its value from the list of numbers specified by range(10). If only one number n is provided to range(n), it will enumerate all numbers from 0 to n-1. If two arguments are provided, the first one determines the starting number, and the second one stands for the terminating number minus one—confusing, right? So, in order to enumerate all numbers from 1 to 10, we have to write range(1,11). Finally, the print function outputs the value of i for each iteration. The end keyword in the print function makes the numbers being printed in one single line, separated by a single white space.1\n\nfor i in range(1,11):\n    print(i, end=\" \")\n\n1 2 3 4 5 6 7 8 9 10 \n\n\nVoilà!"
  },
  {
    "objectID": "python_primer.html#just-in-case",
    "href": "python_primer.html#just-in-case",
    "title": "3  A Python primer",
    "section": "3.3 Just in case",
    "text": "3.3 Just in case\nOften we encounter a situation where we would execute some code only if certain conditions are met. In python, this is done with the if statement. For example, if we want to only print the even numbers in the range from 1 to 10, we could adapt the code from above as follows:\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n\n2 is even\n4 is even\n6 is even\n8 is even\n10 is even\n\n\nYou can read this as “if the remainder of dividing i by 2 is zero, then print ‘i is even’”.\nNow, we could also want to print a similar statement in the case that i is odd:\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is odd\n4 is even\n5 is odd\n6 is even\n7 is odd\n8 is even\n9 is odd\n10 is even\n\n\nAnd, finally, we could also have more than just one condition. An if-statement allows for arbitrary many if-else clauses, with which we can formulate several different conditions by writing elif (shorthand for ‘or else if’):\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n    elif i % 3 == 0:\n        print(i, \"is divisible by 3\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is divisible by 3\n4 is even\n5 is odd\n6 is even\n7 is odd\n8 is even\n9 is divisible by 3\n10 is even\n\n\nWe now know when a number is even and when it is divisible by 3. But what about numbers that are both even and divisible by 3? We just add another condition to the elif statement and enclose each condition in parentheses, so that Python knows how things group together:\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n    elif (i % 2 == 0) and (i % 3 == 0):\n        print(i, \"is even and divisible by 3\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is odd\n4 is even\n5 is odd\n6 is even\n7 is odd\n8 is even\n9 is odd\n10 is even\n\n\nWhy did this not work? The number 6 is even and divisible by 3! The reason is that the three statements (if, elif, and else) are being executed in the order that we wrote them down. That means, that Python will first check for each number whether it is even (and nothing more), and if it is, it will follow the instruction to print it and go on to the next number. So, once we arrived at 6, the programm will only check if the number is even. That is not the desired result and we have to make a little change to it. We will switch the conditions in the if and elif statements:\n\nfor i in range(1,11):\n    if (i % 2 == 0) and (i % 3 == 0):\n        print(i, \"is even and divisible by 3\")\n    elif i % 2 == 0:\n        print(i, \"is even\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is odd\n4 is even\n5 is odd\n6 is even and divisible by 3\n7 is odd\n8 is even\n9 is odd\n10 is even\n\n\nNow it works! Note that new never specified any conditions for the else statement. This is because whatever follows it will be executed in case none of the conditions in if or elif are met."
  },
  {
    "objectID": "python_primer.html#functions",
    "href": "python_primer.html#functions",
    "title": "3  A Python primer",
    "section": "3.4 Functions",
    "text": "3.4 Functions\nWith more and more experience in programming, it is likely that your code will become more and more complex. That means that it will become harder to keep track of what every piece of it is supposed to do. A good strategy to deal with this is to aim for writing code that is modular: it can be broken down into smaller units (modules) that are easier to understand. Moreover, it is sometimes necessary to reuse the same code several times. It would, however, be inefficient to write the same lines over and over again. With your code being modular you can wrap the pieces that you need in several places into a function.\nLet’s look at an example! Assume, your (fairly) complex code involves calculating the sum of the products of two numbers. In Python, we use the + operator to calculate sums and the ** operator to raise a number to a certain power (**2 for the square of a number).\n\nx = 3\ny = 5\n\nsum_of_squares = x**2 + y**2\n\nThe variable sum_of_squares now contains the sum of squares of x=3 and y=5. We can inspect the result by calling the variable:\n\nsum_of_squares \n\n34\n\n\nNow, imagine that you would have to do the same calculation several times for different combinations of values for x and y (and always keeping in mind that this stands in for much more complex examples with several lines of code). We can this code in a function:\n\ndef func_sum_of_squares(x, y):\n    return x**2 + y**2\n\nNow, each time we want to calculate a sum of squares, we can do so by simply invoking\n\nfunc_sum_of_squares(5,4)\n\n41\n\n\nAnd, of course, we could chose a shorter name for the function as well:\n\nf = func_sum_of_squares\n\nf(5,4)\n\n41"
  },
  {
    "objectID": "python_primer.html#libraries-youll-love",
    "href": "python_primer.html#libraries-youll-love",
    "title": "3  A Python primer",
    "section": "3.5 Libraries you’ll love",
    "text": "3.5 Libraries you’ll love\nLuckily, you don’t have to programm all functions by yourself. There is a huge community of Python programmers out there that works and collaborates on several libraries. A library is (more or less) simply a collection of certain functions (and some more, but we don’t get into this here). This means, instead of writing a function yourself, you can rely on functions that someone else has programmed.\n\n\n\n\n\n\nDanger\n\n\n\nWhether a Python library or function does actually do what it promises is another story. Popular libraries with tens of thousands of users are very trust-worthy because you can be almost sure that someone would have noticed erroneous behavior. But it is certainly possible that badly-maintained libraries contain errors. So be prudent when using the code of others.\n\n\nOne of the most popular Python libaries is NumPy for numerical computations. We will rely a lot on the functions in this library, especially in order to draw random samples—more on this later! To use the functions or variables from this library, they have to be imported so that you can use them. There are several ways to do this. For example, you can import the libary entirely:\n\nimport numpy\n\nNow, you can use the (approximated) value of \\(\\pi\\) stored in this library by typing\n\nnumpy.pi\n\n3.141592653589793\n\n\nA different way is to just import everything from the library by writing\n\nfrom numpy import * \n\nHere, the * stands for ‘everything’. Now, to use the value of \\(\\pi\\) we could simply type\n\npi\n\n3.141592653589793\n\n\nThis is, however discouraged for the following reason: imagine we had another library, numpy2 that also stores the value of \\(\\pi\\), but less precisely (e.g. only as 3.14). If we write\n\nfrom numpy import * \nfrom numpy2 import * \n\nWe would have imported the variables holding the value of \\(\\pi\\) from both libraries. But, because they have the same name pi. In this case, pi would equal 3.14 because we imported numpy2 last. This is confusing and shouldn’t be so! To avoid this, it is better to keep references to imported libraries explicit. In order not to have to type too much (we’re all lazy, after all), we can define an alias for the library.\n\nimport numpy as np\nnp.pi\n\n3.141592653589793\n\n\nAll functions of NumPy are now accessible with the prefix np.. You can choose any alias when importing a library (it can even by longer than the library name) but certain conventions have emerged that you’re encouraged to follow. Importing the most commonly used Python libraries for data-science tasks (“The data science triad”), use the following:\n\nimport numpy as np # for numerical computations\nimport pandas as pd # for tabular data \nimport matplotlib.pyplot as plt # for data visualization\n\nWe will use all three of them in the following chapters and you’ll learn to love them."
  },
  {
    "objectID": "randomness.html#random-draws-from-a-bag",
    "href": "randomness.html#random-draws-from-a-bag",
    "title": "4  Randomness and order",
    "section": "4.1 Random draws from a bag",
    "text": "4.1 Random draws from a bag\nLet’s try this in Python. We will use the random modul of the NumPy library:\n\nimport numpy as np\n\nbag = range(4)\nball = np.random.choice(bag)\n\nprint(ball)\n\n2\n\n\nIf this draw were really random, we would expect that each number is equally likely. We can test this by repeating this procedure again and again, tallying the result each time.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nsamples = []\n\nfor i in range(1000):\n    ball = np.random.choice(bag)\n    samples.append(ball)\n\ns = pd.Series(samples)\n\nprint(s.head(10))\nprint(s.value_counts().sort_index())\n\n0    2\n1    3\n2    2\n3    2\n4    1\n5    3\n6    3\n7    2\n8    2\n9    2\ndtype: int64\n0    253\n1    244\n2    249\n3    254\ndtype: int64\n\n\nThe number are not exactly the same but pretty close. If we would continue sampling from our bag, they would get more and more similar to one another. It is easier to understand this by visualizing it.\n\ns.value_counts().sort_index().plot(kind=\"bar\")\nplt.show()\n\n\n\n\nNow, what if the numbers in the bag were not just numbered, but had different colors? Let’s assume we have another bag, bag2, with 4 balls, three brown, one blue:\n\nbag2 = [\"brown\", \"brown\", \"brown\", \"blue\"]\n\nsamples2 = []\n\nfor i in range(1000):\n    ball = np.random.choice(bag2)\n    samples2.append(ball)\n\ns2 = pd.Series(samples2)\n\nprint(s2.head(10))\nprint(s2.value_counts().sort_index())\n\ns2.value_counts().sort_index().plot(kind=\"bar\")\nplt.show()\n\n0    brown\n1    brown\n2     blue\n3    brown\n4     blue\n5    brown\n6     blue\n7    brown\n8    brown\n9    brown\ndtype: object\nblue     247\nbrown    753\ndtype: int64\n\n\n\n\n\nThis is remarkable: by randomly (uniformly) drawing from the second bag, the frequencies of all samples approach the ratio of brown to blue balls (3:1)!"
  },
  {
    "objectID": "randomness.html#composing-random-melodies",
    "href": "randomness.html#composing-random-melodies",
    "title": "4  Randomness and order",
    "section": "4.2 Composing random melodies",
    "text": "4.2 Composing random melodies\nSince this book is about music, let’s see how we can use randomness to create (a resemblance of) music. For instance, we can ‘compose’ a random melody by using only the white keys on a piano within some octave:\n\nnotes = list(\"CDEFGAB\")\nmelody = np.random.choice(notes, size=10)\nprint(melody, end=\" \")\n\n['E' 'C' 'G' 'A' 'A' 'A' 'B' 'A' 'B' 'A'] \n\n\nWe composed a little melody by randomly drawing a note from the list of notes. This is also called sampling. Note that some notes repeat, showing that we sample with replacement: after each draw, the note is put back in the bag, so to speak. Of course, there are many things that we would have to generate, too, to make this a real melody. For instance, we do not know the duration of any of these notes, we don’t know the meter nor the key, we don’t know the tempo or volume, and so on. But our goal here is not to create a beautiful piece of music, but rather to show how we can use randomness to generate something.\nAs you might remember from the previous chapter, we can also write a function to do this, so that we can perform this operation (composition of a random melody) more easily, while at the same time having more control over it through its parameters. The following function does exactly this, having only one parameter that controlls the length of the melody (the number of notes to be sampled).\n\ndef melody(n):\n    notes = list(\"CDEFGAB\")\n    return np.random.choice(notes, size=n)\n\nWe can now use this function to easily create random melodies of different lengths:\n\nprint(melody(7))\n\n['F' 'A' 'B' 'G' 'C' 'D' 'G']\n\n\n\nprint(melody(12))\n\n['E' 'C' 'E' 'E' 'A' 'C' 'B' 'D' 'E' 'C' 'E' 'D']"
  },
  {
    "objectID": "randomness.html#random-bach",
    "href": "randomness.html#random-bach",
    "title": "4  Randomness and order",
    "section": "4.3 Random Bach",
    "text": "4.3 Random Bach\nFour-part writing is a core part of Western composition history. Here, we will build a mock version of a four-part chorale by randomly generating each voice and putting them together in a table. Doing so will show you how you can create tables, which we will need later on. The most popular way to work with tables in Python is by using the pandas library. In pandas, tables are called ‘data frames’, and there is a DataFrame object to represent tables. Let’s see how we could create a four-part homophonic chorale with eight ‘chords’:\n\nimport pandas as pd\n\nchorale = pd.DataFrame({\n    \"S\" : melody(n=8),\n    \"A\" : melody(n=8),\n    \"T\" : melody(n=8),\n    \"B\" : melody(n=8)\n})\n\nThe variable chorale now stores our little composition and we can inspect it:\n\nchorale\n\n\n\n\n\n  \n    \n      \n      S\n      A\n      T\n      B\n    \n  \n  \n    \n      0\n      A\n      G\n      D\n      D\n    \n    \n      1\n      G\n      G\n      B\n      D\n    \n    \n      2\n      A\n      G\n      E\n      E\n    \n    \n      3\n      A\n      C\n      E\n      F\n    \n    \n      4\n      G\n      B\n      A\n      A\n    \n    \n      5\n      B\n      A\n      F\n      A\n    \n    \n      6\n      A\n      F\n      D\n      F\n    \n    \n      7\n      C\n      E\n      E\n      C\n    \n  \n\n\n\n\nHere we have generated each voice using the melody function. We can use it to create a new function that will directly give us a new chorale of a certain length:\n\ndef chorale(n):\n    df = pd.DataFrame({\n        \"S\" : melody(n=n),\n        \"A\" : melody(n=n),\n        \"T\" : melody(n=n),\n        \"B\" : melody(n=n)\n    })\n\n    return df\n\n\nmy_chorale = chorale(n=12)\nmy_chorale\n\n\n\n\n\n  \n    \n      \n      S\n      A\n      T\n      B\n    \n  \n  \n    \n      0\n      G\n      C\n      B\n      A\n    \n    \n      1\n      G\n      G\n      E\n      F\n    \n    \n      2\n      D\n      F\n      G\n      E\n    \n    \n      3\n      A\n      B\n      C\n      A\n    \n    \n      4\n      A\n      G\n      B\n      C\n    \n    \n      5\n      D\n      D\n      F\n      A\n    \n    \n      6\n      D\n      D\n      G\n      E\n    \n    \n      7\n      F\n      D\n      F\n      A\n    \n    \n      8\n      B\n      E\n      A\n      E\n    \n    \n      9\n      C\n      F\n      F\n      F\n    \n    \n      10\n      B\n      C\n      A\n      G\n    \n    \n      11\n      B\n      G\n      B\n      G\n    \n  \n\n\n\n\nIt will look a bit closer to musical notation if we transpose the data frame by using the .T attribute:\n\nmy_chorale.T\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n  \n  \n    \n      S\n      G\n      G\n      D\n      A\n      A\n      D\n      D\n      F\n      B\n      C\n      B\n      B\n    \n    \n      A\n      C\n      G\n      F\n      B\n      G\n      D\n      D\n      D\n      E\n      F\n      C\n      G\n    \n    \n      T\n      B\n      E\n      G\n      C\n      B\n      F\n      G\n      F\n      A\n      F\n      A\n      B\n    \n    \n      B\n      A\n      F\n      E\n      A\n      C\n      A\n      E\n      A\n      E\n      F\n      G\n      G"
  },
  {
    "objectID": "randomness.html#accessing-data",
    "href": "randomness.html#accessing-data",
    "title": "4  Randomness and order",
    "section": "4.4 Accessing data",
    "text": "4.4 Accessing data\nHaving the variable my_chorale store our data frame, this is how we can access individual voices:\n\nmy_chorale[\"T\"]\n\n0     B\n1     E\n2     G\n3     C\n4     B\n5     F\n6     G\n7     F\n8     A\n9     F\n10    A\n11    B\nName: T, dtype: object\n\n\nYou can verify that it is the same ‘melody’ as above in the chorale. If we want a specific note from this voice, say the fifth one, we can access is this way:\n\nmy_chorale[\"T\"][4]\n\n'B'\n\n\nWe first select the “T” column, and then select the fifth element (remember that we start counting at 0, so we need to insert 4 to get the fifth). We can also get entire ranges of a voice:\n\nmy_chorale[\"A\"][4:8]\n\n4    G\n5    D\n6    D\n7    D\nName: A, dtype: object\n\n\nThis gives us the fifths to ninth note in the Alto voice. If we want to apply the same logic also to column ranges, we have to write it a bit differently using the .loc() method for localising data:\n\nmy_chorale.loc[1:3, \"S\":\"A\"]\n\n\n\n\n\n  \n    \n      \n      S\n      A\n    \n  \n  \n    \n      1\n      G\n      G\n    \n    \n      2\n      D\n      F\n    \n    \n      3\n      A\n      B\n    \n  \n\n\n\n\n.loc() takes two arguments: the rows (or row range), and the columns (or column range). We can use it to ‘slice’ our data frame in order to get specific portions of it."
  },
  {
    "objectID": "models.html#a-simple-example",
    "href": "models.html#a-simple-example",
    "title": "5  Models",
    "section": "5.1 A simple example",
    "text": "5.1 A simple example\nModels are meant to be abstract representations of (some part of) reality. They necessarily need to be simpler than reality, simple enough so that we can understand them, but close enough to reality so that they are actually useful to us.\nWhy not having more complex models that accurately represent reality? Well, first of all that is a very difficult endeavour. But, more importantly:\n\nNo model is ever a complete recreation of reality. That would be pointless: we would have replaced a complex, incomprehensible reality with a complex, incomprehensible model. Instead, models are useful because of their simplicity. (Acerbi et al., 2022, Introduction)\n\nSo what does that mean in practice? I will try to demonstrate this with an admittedly boring but hopefully illustrative example. Assume we want to model the entire area of this (fictional) country:\n\n\n\nThe outline of a fictional country.\n\n\nMaybe the most simple, albeit naive approach would be to say that the total area of this country can be modeled with a square. A square is a very parsimonious model: in order to describe its area, only one parameter is needed, its side length.\n\n\n\nA square.\n\n\nWhat’s more, we can even give a precise mathematical formula for the area of our fictional country under the square model:\n\\[M_1(a) = a^2\\]\nIn Python code, we could express this model as the following function:\n\ndef square_model(a):\n    return a**2\n\nOf course, this is not a good model of the area of the country. But one of the strengths of formal models is that they are unquivocal. There might be situations, in which the rough estimate of the square model is actually sufficient for our purpose. So why use a more complex model if the simplest one does the job?\nMost of the time, however, such a simplistic model will not suffice and we need to invest brain power to come up with a better one.1 The following model is one way to improve upon our first mode:\n\n\n\nA rectangle.\n\n\nThe shape of the rectangle seems to fit our country’s outline better. Hooray! As for the square model, we do have a mathematical formuly to describe our rectangle model:\n\\[M_2(a,b) = a \\cdot b\\]\nIn Python code:\n\ndef rectangle(a,b):\n    return a * b\n\nBut this improvement comes at a price: instead of having only one parameter, a, we now have two, a and b. Our model has instantly become twice as complex!\nModeling is at the core of social science and there are many good text about this topic (McElreath, 2020; Smaldino, 2017; Smaldino, 2023). In the humanities, there are fewer approaches taking modeling seriously, but they are growing in number (Finkensiep et al., forthcoming; Piotrowski, 2019).\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/\n\n\nBentley, R. A., Hahn, M. W., & Shennan, S. J. (2004). Random drift and culture change. Proceedings of the Royal Society of London. Series B: Biological Sciences, 271(1547), 1443–1450. https://doi.org/10.1098/rspb.2004.2746\n\n\nBishop, C. M. (2012). Model-based machine learning. Philosophical Transactions of The Royal Society A, 0222(371). https://doi.org/10.1098/rsta.2012.0222\n\n\nFarrell, S., & Lewandowsky, S. (2018). Computational Modeling of Cognition and Behavior. Cambridge University Press. https://doi.org/10.1017/CBO9781316272503\n\n\nFinkensiep, C., Neuwirth, M., & Rohrmeier, M. (forthcoming). Music Theory and Model-driven Corpus Research. In D. Shanahan, J. A. Burgoyne, & I. Quinn (Eds.), Oxford Handbook of Music and Corpus Studies. Oxford University Press.\n\n\nHoning, H. (2006). Computational Modeling of Music Cognition: A Case Study on Model Selection. Music Perception, 23(5), 365–376. https://doi.org/10.1525/mp.2006.23.5.365\n\n\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and STAN (Second). Chapman and Hall/CRC.\n\n\nPiotrowski, M. (2019). Historical Models and Serial Sources. Journal of European Periodical Studies, 4(1), 8–18. https://doi.org/10.21825/jeps.v4i1.10226\n\n\nSmaldino, P. E. (2017). Models Are Stupid, and We Need More of Them. In R. R. Vallacher, S. J. Read, & A. Nowak (Eds.), Computational Social Psychology (First, pp. 311–331). Routledge. https://doi.org/10.4324/9781315173726-14\n\n\nSmaldino, P. E. (2023). Modeling Social Behavior: Mathematical and Agent-Based Models of Social Dynamics and Cultural Evolution. Princeton University Press."
  },
  {
    "objectID": "chapter03.html#creating-a-conceptual-understanding",
    "href": "chapter03.html#creating-a-conceptual-understanding",
    "title": "6  Unbiased transmission",
    "section": "6.1 Creating a conceptual understanding",
    "text": "6.1 Creating a conceptual understanding\nIt is a useful exercise to imagine first what we want to implement. This way we can check whether we have a good conceptual understanding, which will help us to write the code more clearly and make fewer mistakes. The following figure shows in the upper panel a scenario, in which there is a population of N individuals present in each generation. Those individuals are uniquely characterized by one of two traits: whether they like opera (salmon color) or not (grey). This visualization captures all assumptions we make for our first model:\n\nThe number of individuals per generation does not change.\nGenerations are disjunct, there is no overlap.\nIn the first generation, opera preference is randomly assigned to individuals.\nIn each subsequent generation, each individual randomly picks an ‘ancestor’ from the previous generation and blindly copies that individual’s preference for opera.\n\nThe models throughout this book assume that invidivuals in one generation learn from individuals of (only) the previous generation by “picking” an older individual and adopting its traits according to some probabilistic rules built into the model. This means an arrow from individual n in generation t to an individual m in generation t + 1 should be interpreted as: “Individual m learns from individual m” and not the other way around (because information is flowing from generation t to t+1).\n\n\n\nA conceptual depiction of unbiased transmission.\n\n\nThe lower plot shows, at each time, point the percentage of individuals having a preference for opera. A logical consequence of this process is that each transmission chain (each sequence of directly connected arrows) will pass through one and only one trait (color).\nWe can make further observations from this initial example. In the first generation, there are two individuals who do like opera and three who don’t. In subsequent generations, the proporation of these two traits changes. It is not monotonic (it goes both up and down), as the red line shows. But in the sixth generation, everything changes. Individuals in the sixth generation could inherit their trait from the one individual in generation 5 that likes opera. But because they randomly pick their ancestors, that individual is not among the ancestors. Consequently, no individual in generation 6 likes opera. This also means that, from now on, now one will ever like opera again. Opera fans have become extinct.\nWe can see that transmission of information is still going on: there are arrows between generations, so individuals still receive information from the previous generation. But nothing changes. That means that we do have transmission of information, but we would not anymore speak of it as cultural evolution, since the first fundamental criterion (see Chapter X), variation, is not fulfilled.\nNote also, that some ‘traditions’ are likewise not continued. For example, following the transmission chain of individual 0 in the first generation to individual 2 in the second generation to individual 3 in the third generation, we see that this latter individual is not picked as an ancestor by anyone from the next generation. This individual (and all of its ancestors) have no more impact on future generations. Importantly, however, we are not interested in individual fates, but rather in population-level statistics. That is why the lower plot only traces the proportion of individuals having a preference for opera."
  },
  {
    "objectID": "chapter03.html#simulating-a-population",
    "href": "chapter03.html#simulating-a-population",
    "title": "6  Unbiased transmission",
    "section": "6.2 Simulating a population",
    "text": "6.2 Simulating a population\nWith this conceptual understanding in mind, we will now look at how we can reproduce this model of unbiased transmission. Since we are assuming that new individuals randomly pick an ancestor, we should not assume that our results will be exactly the same as above, but they should nonetheless be qualitatively similar.\n\nN = 100\nt_max = 100\n\n\n\n\n\n\n\nNote\n\n\n\nIn general, we use the variable t to designate generation counts.\n\n\nNow we create a variable population that will store the data about our simulated population. This population has either of two traits \"A\" and \"B\", with a certain probability. We store all of this in a so-called ‘data frame’, which is a somewhat fancy, Pandas-specific term for a table.\n\npopulation = pd.DataFrame(\n    {\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True)}\n)\n\nLet’s take this code apart to understand it better. From the Pandas library, which we imported as the alias pd, we create a DataFrame object. The data contained in this the data frame population is specified via a dictionary that has \"trait\" as its key and a fairly complex expression starting with the random number generator rng as its value. What this value says is, from the list [\"A\", \"B\"] choose randomly N instances with replacement (if replace were set to False, we could at most sample 2 values from the list). So, the data frame population should contain 100 randomly sampled values of A’s and B’s. Let’s confirm this:\n\npopulation.head()\n\n\n\n\n\n  \n    \n      \n      trait\n    \n  \n  \n    \n      0\n      A\n    \n    \n      1\n      B\n    \n    \n      2\n      B\n    \n    \n      3\n      A\n    \n    \n      4\n      A\n    \n  \n\n\n\n\nAs you can see, population stores a table (many of the 100 rows are omitted here for display reasons) and a single column called ‘trait’. The .head() method appended to the population data frame shows restricts the output to only the first 5 rows (0 through 4). Each row in the ‘trait’ column contains either the value A or B. To the left of the data frame you can see the numbers of rows explicitly spelled out. This is called the data frame’s index.\n\n\n\n\n\n\nNote\n\n\n\nA and B are just placeholder names for any of two mutually exclusive cultural traits. These could be, for example, preference for red over white whine (ignoring people who like rosé as well as people who have no preference). You see already here that this is a massive oversimplification of actual taste preferences. The point here is not to construct a plausible model but rather to gradually build up a simple one in order to understand well its inner workings.\nIt will help to pause for a moment and to think of other examples that “A” and “B” could stand for. Can you come up with a music-related one?\n\n\nFor instance, we could say that the mutually exclusive traits “A” and “B” correspond to “Individual likes opera” and “Individual doesn’t like opera”. People are often opinionated about opera, so we will stick to this example for the remainder of this part of the book. But be encouraged to try to transfer the following to different hypothetical scenarios.\n\n\n\nScene from “What’s opera, doc?” (1957).\n\n\nBack to our artificial population. We can count the number of A’s and B’s amongst the individuals as follows:\n\npopulation[\"trait\"].value_counts()\n\nB    52\nA    48\nName: trait, dtype: int64\n\n\nYou can read the above code as “From the population table, select the ‘trait’ colum and count its values.”. Since there were only two values to sample from and they were randomly (uniformly) sampled, the number of A’s and the number of B’s should be approximately equal. We can obtain their relative frequencies by adding setting the normalize keyword to True:\n\npopulation[\"trait\"].value_counts(normalize=True)\n\nB    0.52\nA    0.48\nName: trait, dtype: float64"
  },
  {
    "objectID": "chapter03.html#tracing-cultural-change",
    "href": "chapter03.html#tracing-cultural-change",
    "title": "6  Unbiased transmission",
    "section": "6.3 Tracing cultural change",
    "text": "6.3 Tracing cultural change\nWe now create a second data frame output in which we will store the output of our model. This data frame has two columns: generation, which is the number of the simulated generation, and p which stands for “the probability of an individual of having trait A”.\n\noutput = pd.DataFrame(\n    {\n        \"generation\": np.arange(t_max, dtype=int), \n        \"p\": [np.nan] * t_max \n    }\n)\n\nThe generation column contains all numbers from 0 to t_max - 1. Because we count the numbers of generations (rather than assuming a time-continuous process), we specified that numbers in this column have to be intergers (dtype=int). The values for the p column must look cryptic. It literally says: put the np.nan value t_max times into the p colum. np.nan stands for “not a number” (from the NumPy library), since we haven’t assigned any values to this probability yet.\n\noutput.head()\n\n\n\n\n\n  \n    \n      \n      generation\n      p\n    \n  \n  \n    \n      0\n      0\n      NaN\n    \n    \n      1\n      1\n      NaN\n    \n    \n      2\n      2\n      NaN\n    \n    \n      3\n      3\n      NaN\n    \n    \n      4\n      4\n      NaN\n    \n  \n\n\n\n\nDon’t worry that both the index and the ‘generation’ column contain all numbers from 0 to 99. We need this later when things become more involved.\nAs the saying goes, from nothing comes nothing, so we have to start somewhere, meaning that we need to assume that the initial probability of having trait A in our population is an actual number. The most sensible thing is to start with the proportions of A and B in our sampled population as a starting value.\nSo, we approximate the probability of an individual having trait A with the relative frequency of trait A in the population:\n\npopulation[\"trait\"].value_counts(normalize=True)[\"A\"]\n\n0.48\n\n\nYou already know this code from above, we just added the [\"A\"] part at the end to select only the relative frequencies of trait A. We want to set this as the value of p of the first generation. This can be achieved with the .loc (location) method:\n\noutput.loc[0, \"p\"] = population[\"trait\"].value_counts(normalize=True)[\"A\"]\n\nIn words, this reads: “Set location 0 (first row) in the p column of the output data frame to the relative frequency of the trait ‘A’ in the population.”"
  },
  {
    "objectID": "chapter03.html#iterating-over-generations",
    "href": "chapter03.html#iterating-over-generations",
    "title": "6  Unbiased transmission",
    "section": "6.4 Iterating over generations",
    "text": "6.4 Iterating over generations\nRecall that we are trying to observe cultural change over the course of t_max = 100 generations. We thus simply repeat what we just did for the first generation: based on the relative frequencies of A’s and B’s in the previous generation, we sample the traits of 100 new individuals for the next generation.\n\nfor t in range(1, t_max):\n    # Copy the population data frame to `previous_population`\n    previous_population = population.copy()\n  \n    # Randomly copy from previous generation's individuals\n    new_population = previous_population[\"trait\"].sample(N, replace=True).to_frame()\n    \n    # Get p and put it into the output slot for this generation t\n    output.loc[t, \"p\"] = new_population[ new_population[\"trait\"] == \"A\"].shape[0] / N\n\nThis procedure assignes a probability of having trait “A” for each generation (each row of the p colum is filled now):\n\noutput.head()\n\n\n\n\n\n  \n    \n      \n      generation\n      p\n    \n  \n  \n    \n      0\n      0\n      0.48\n    \n    \n      1\n      1\n      0.52\n    \n    \n      2\n      2\n      0.44\n    \n    \n      3\n      3\n      0.46\n    \n    \n      4\n      4\n      0.55\n    \n  \n\n\n\n\nTo make things easier, we wrap the above code in a function that we’ll call unbiased_transmission that can take different values for the population size N and number of generations t_max as parameters. The code below is exactly the same as above.\n\ndef unbiased_transmission_1(N, t_max):\n    population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True)})\n\n    output = pd.DataFrame({\"generation\": np.arange(t_max, dtype=int), \"p\": [np.nan] * t_max })\n\n    output.loc[0, \"p\"] = population[ population[\"trait\"] == \"A\"].shape[0] / N\n\n    for t in range(1, t_max):\n        # Copy the population tibble to previous_population tibble\n        previous_population = population.copy()\n    \n        # Randomly copy from previous generation's individuals\n        new_population = previous_population[\"trait\"].sample(N, replace=True).to_frame()\n        \n        # Get p and put it into the output slot for this generation t\n        output.loc[t, \"p\"] = new_population[ new_population[\"trait\"] == \"A\"].shape[0] / N\n    \n    return output\n\n\ndata_model = unbiased_transmission_1(N=100, t_max=200)\n\n\ndef plot_single_run(data_model):\n    data_model[\"p\"].plot(ylim=(0,1))\n\n\nplot_single_run(data_model)\n\n\n\n\nSingle run of the unbiased transmission model for a population of \\(N=100\\) individuals and \\(t_{max}=200\\) generations.\n\n\n\n\n\ndata_model = unbiased_transmission_1(N=10_000, t_max=200)\n\n\nplot_single_run(data_model)\n\n\n\n\nSingle run of the unbiased transmission model for a population of \\(N=10,000\\) individuals and \\(t_{max}=200\\) generations.\n\n\n\n\n\n\nNow, let’s adapt the code somewhat.\n\ndef unbiased_transmission_2(N, t_max, r_max):\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True)})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n\n            # Randomly compy from previous generation \n            population = population[\"trait\"].sample(N, replace=True).to_frame()\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output\n\n\nunbiased_transmission_2(100, 100, 3).head()\n\n\n\n\n\n  \n    \n      \n      generation\n      p\n      run\n    \n  \n  \n    \n      0\n      0\n      0.45\n      0\n    \n    \n      1\n      1\n      0.48\n      0\n    \n    \n      2\n      2\n      0.45\n      0\n    \n    \n      3\n      3\n      0.48\n      0\n    \n    \n      4\n      4\n      0.52\n      0\n    \n  \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhy could we append .head() to the unbiased_transmission_2 function?\n\n\n\ndata_model = unbiased_transmission_2(N=100, t_max=200, r_max=5)\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\nplot_multiple_runs(data_model)\n\n\n\n\nMultiple runs of the unbiased transmission model for a population of \\(N=100\\) individuals, with average (black line).\n\n\n\n\n\ndata_model = unbiased_transmission_2(N=10_000, t_max=200, r_max=5)\n\n\nplot_multiple_runs(data_model)\n\n\n\n\nMultiple runs of the unbiased transmission model for a population of \\(N=10,000\\) individuals, with average (black line).\n\n\n\n\n\ndef unbiased_transmission_3(N, p_0, t_max, r_max):\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population\n\n            # Randomly compy from previous generation \n            population = population[\"trait\"].sample(N, replace=True).to_frame()\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output\n\n\ndata_model = unbiased_transmission_3(10_000, p_0=.2, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/"
  },
  {
    "objectID": "chapter04.html",
    "href": "chapter04.html",
    "title": "7  Unbiased mutation",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is based on “Chapter 2: Unbiased and biased mutation” in Acerbi et al. (2022).\n\n\n\nimport numpy as np\nrng = np.random.default_rng()\n\nimport pandas as pd\n\n\ndef unbiased_mutation(N, mu, p_0, t_max, r_max):\n    # Create an output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n            \n            # Determine \"mutant\" individuals\n            mutate = rng.choice([True, False], size=N, p=[mu, 1-mu], replace=True)\n\n            # TODO: Something is off here! Changing the order of the conditions affects\n            # the result. Should be constant with random noise but converges to either A or B\n\n            # If there are \"mutants\" from A to B \n            conditionA = mutate & (previous_population[\"trait\"] == \"A\")\n            if conditionA.sum() > 0:\n                population.loc[conditionA, \"trait\"] = \"B\"\n\n            # If there are \"mutants\" from B to A\n            conditionB = mutate & (previous_population[\"trait\"] == \"B\")\n            if conditionB.sum() > 0:\n                population.loc[conditionB, \"trait\"] = \"A\"\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\ndata_model = unbiased_mutation(N=100, mu=.05, p_0=0.5, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = unbiased_mutation(N=100, mu=.05, p_0=0.1, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/"
  },
  {
    "objectID": "biased_mutation.html",
    "href": "biased_mutation.html",
    "title": "8  Biased mutation",
    "section": "",
    "text": "import numpy as np\nrng = np.random.default_rng()\n\nimport pandas as pd\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\ndef biased_mutation(N, mu_b, p_0, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n            \n            # Determine \"mutant\" individuals\n            mutate = rng.choice([True, False], size=N, p=[mu_b, 1-mu_b], replace=True)\n\n            # TODO: Something is off here! Changing the order of the conditions affects\n            # the result. Should be constant with random noise but converges to either A or B\n\n            # If there are \"mutants\" from B to A\n            conditionB = mutate & (previous_population[\"trait\"] == \"B\")\n            if conditionB.sum() > 0:\n                population.loc[conditionB, \"trait\"] = \"A\"\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndata_model = biased_mutation(N = 100, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = biased_mutation(N = 10000, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model <- biased_mutation(N = 10000, mu_b = 0.1, p_0 = 0, t_max = 200, r_max = 5)\nplot_multiple_runs(data_model)"
  },
  {
    "objectID": "chapter05.html",
    "href": "chapter05.html",
    "title": "9  Biased transmission: direct bias",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is based on “Chapter 3: Biased transmission: direct bias” in Acerbi et al. (2022).\n\n\n\nimport numpy as np\nrng = np.random.default_rng()\nimport pandas as pd\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\ndef biased_transmission_direct(N, s_a, s_b, p_0, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n\n            # For each individual, pick a random individual from the previous generation\n            demonstrator_trait = previous_population[\"trait\"].sample(N, replace=True).reset_index()\n            \n            # Biased probabilities to copy\n            copy_a = rng.choice([True, False], size=N, replace=True, p=[s_a, 1 - s_a])\n            copy_b = rng.choice([True, False], size=N, replace=True, p=[s_b, 1 - s_b])\n\n            # If the demonstrator has trait A and the individual wants to copy A, then copy A\n            condition = copy_a & (demonstrator_trait[\"trait\"] == \"A\")\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"A\"\n\n            # If the demonstrator has trait B and the individual wants to copy B, then copy B\n            condition = copy_b & (demonstrator_trait[\"trait\"] == \"B\")\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"B\"\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndata_model = biased_transmission_direct(N=10_000, s_a=.1, s_b=0, \n                                         p_0=.01, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = biased_transmission_direct(N=10_000, s_a=.6, s_b=.5, \n                                         p_0=.01, t_max=150, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = biased_transmission_direct(N=10_000, s_a=.2, s_b=0, \n                                         p_0=.01, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/"
  },
  {
    "objectID": "chapter06.html",
    "href": "chapter06.html",
    "title": "10  Biased transmission: frequency-dependent indirect bias",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is based on “Chapter 4: Biased transmission: frequency-dependent indirect bias” in Acerbi et al. (2022).\n\n\n\nimport numpy as np \nrng = np.random.default_rng()\n\nimport pandas as pd\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\nN = 100\np_0 = .5\nD = 1.\n\n\n# Create first generation\npopulation = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1-p_0])})\n\n\n# Create a DataFrame with a set of 3 randomly-picked demonstrators for each agent\n\ndemonstrators = pd.DataFrame({\n    \"dem1\" : population[\"trait\"].sample(N, replace=True).values,\n    \"dem2\" : population[\"trait\"].sample(N, replace=True).values,\n    \"dem3\" : population[\"trait\"].sample(N, replace=True).values\n})\n\n\n# Visualize the DataFrame\ndemonstrators.head()\n\n\n\n\n\n  \n    \n      \n      dem1\n      dem2\n      dem3\n    \n  \n  \n    \n      0\n      B\n      B\n      B\n    \n    \n      1\n      B\n      B\n      B\n    \n    \n      2\n      A\n      A\n      A\n    \n    \n      3\n      A\n      B\n      A\n    \n    \n      4\n      B\n      A\n      B\n    \n  \n\n\n\n\n\n# Get the number of A's in each 3-demonstrator combination\nnum_As = (demonstrators == \"A\").apply(sum, axis=1)\nnum_As.head()\n\n0    0\n1    0\n2    3\n3    2\n4    1\ndtype: int64\n\n\n\n# For 3-demonstrator combinations with all A's, set to A\npopulation[ num_As == 3 ] = \"A\"\n# For 3-demonstrator combinations with all B's, set to B\npopulation[ num_As == 0 ] = \"B\"\n\n\nprob_majority = rng.choice([True, False], p=[(2/3 + D/3), 1-(2/3 + D/3)], size=N, replace=True)\nprob_minority = rng.choice([True, False], p=[(1/3 + D/3), 1-(1/3 + D/3)], size=N, replace=True)\n\n\n# 3-demonstrator combinations with two As and one B\ncondition = prob_majority & (num_As == 2)\nif condition.sum() > 0:\n    population.loc[condition, \"trait\"] = \"A\"\ncondition = ~prob_majority & (num_As == 2)\nif condition.sum() > 0:\n    population.loc[condition, \"trait\"] = \"B\"\n\n# 3-demonstrator combinations with two B's and one A\ncondition = ~prob_minority & (num_As == 1)\nif condition.sum() > 0:\n    population.loc[condition, \"trait\"] = \"A\"\ncondition = prob_minority & (num_As == 1)\nif condition.sum() > 0:\n    population.loc[condition, \"trait\"] = \"B\"\n\n\ndemonstrators[\"new_trait\"] = population[\"trait\"]\ndemonstrators.head()\n\n\n\n\n\n  \n    \n      \n      dem1\n      dem2\n      dem3\n      new_trait\n    \n  \n  \n    \n      0\n      B\n      B\n      B\n      B\n    \n    \n      1\n      B\n      B\n      B\n      B\n    \n    \n      2\n      A\n      A\n      A\n      A\n    \n    \n      3\n      A\n      B\n      A\n      A\n    \n    \n      4\n      B\n      A\n      B\n      B\n    \n  \n\n\n\n\n\ndef conformist_transmission(N, p_0, D, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            demonstrators = pd.DataFrame({\n                \"dem1\" : population[\"trait\"].sample(N, replace=True).values,\n                \"dem2\" : population[\"trait\"].sample(N, replace=True).values,\n                \"dem3\" : population[\"trait\"].sample(N, replace=True).values\n            })\n\n            # Get the number of A's in each 3-demonstrator combination\n            num_As = (demonstrators == \"A\").apply(sum, axis=1)\n\n            # For 3-demonstrator combinations with all A's, set to A\n            population[ num_As == 3 ] = \"A\"\n            # For 3-demonstrator combinations with all A's, set to A\n            population[ num_As == 3 ] = \"A\"\n            # For 3-demonstrator combinations with all B's, set to B\n            population[ num_As == 0 ] = \"B\"\n\n            prob_majority = rng.choice([True, False], p=[(2/3 + D/3), 1-(2/3 + D/3)], size=N, replace=True)\n            prob_minority = rng.choice([True, False], p=[(1/3 + D/3), 1-(1/3 + D/3)], size=N, replace=True)\n\n            # 3-demonstrator combinations with two As and one B\n            condition = prob_majority & (num_As == 2)\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"A\"\n            condition = ~prob_majority & (num_As == 2)\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"B\"\n\n            # 3-demonstrator combinations with two B's and one A\n            condition = prob_minority & (num_As == 1)\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"A\"\n            condition = ~prob_minority & (num_As == 1)\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"B\"\n            \n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output\n\n\ndata_model = conformist_transmission(N=1_000, p_0 = 0.5, D = 1, t_max = 50, r_max = 10)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/"
  },
  {
    "objectID": "chapter07.html",
    "href": "chapter07.html",
    "title": "11  Biased transmission: influencer-based indirect bias",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is based on “Chapter 5: Biased transmission: demonstrator-based indirect bias” in Acerbi et al. (2022).\n\n\nInstead of calling them demonstrators, we will call them influencers.\n\n\n\nAn influencer on a popular video platform.\n\n\n\nimport numpy as np \nrng = np.random.default_rng()\n\nimport pandas as pd\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\nN = 100\np_0 = 0.5\np_s = 0.05\n\n\npopulation = pd.DataFrame({\n    \"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1-p_0]),\n    \"status\": rng.choice([\"high\", \"low\"], size=N, replace=True, p=[p_s, 1-p_s])\n})\n\n\npopulation.head()\n\n\n\n\n\n  \n    \n      \n      trait\n      status\n    \n  \n  \n    \n      0\n      B\n      low\n    \n    \n      1\n      A\n      low\n    \n    \n      2\n      B\n      low\n    \n    \n      3\n      B\n      low\n    \n    \n      4\n      A\n      low\n    \n  \n\n\n\n\n\np_low = 0.01\np_influencer = np.ones(N)\np_influencer[ population[\"status\"] == \"low\" ] = p_low\n\n\nif sum(p_influencer) > 0:\n    ps = p_influencer / p_influencer.sum()\n    influencer_index = rng.choice(np.arange(N), size=N, p=ps, replace=True)\n    population[\"trait\"] = population.loc[influencer_index, \"trait\"].values\n\n\ndef biased_transmission_influencer(N, p_0, p_s, p_low, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n    \n    for r in range(r_max):\n            # Create first generation\n            population = pd.DataFrame({\n                \"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1-p_0]),\n                \"status\": rng.choice([\"high\", \"low\"], size=N, replace=True, p=[p_s, 1-p_s])\n            })\n            \n            # Assign copying probabilities based on individuals' status\n            p_influencer = np.ones(N)\n            p_influencer[population[\"status\"] == \"low\"] = p_low\n            \n            # Add first generation's p for run r\n            output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n            \n            for t in range(1, t_max):\n                # Copy individuals to previous_population DataFrame\n                previous_population = population.copy()\n                \n                # Copy traits based on status\n                if sum(p_influencer) > 0:\n                    ps = p_influencer / p_influencer.sum()\n                    influencer_index = rng.choice(np.arange(N), size=N, p=ps, replace=True)\n                    population[\"trait\"] = population.loc[influencer_index, \"trait\"].values\n                \n                # Get p and put it into output slot for this generation t and run r\n                output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n                \n    return output\n\n\ndata_model = biased_transmission_influencer(N=100, p_s=0.05, p_low=0.0001, p_0=0.5, t_max=50, r_max=10)\n\n\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = biased_transmission_influencer(N=10_000, p_s=0.005, p_low=0.0001, p_0=0.5, t_max=200, r_max=10)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndef biased_transmission_influencer_2(N, p_0, p_s, p_low, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n    \n    ... # TODO\n    \n    return output\n\n\ndata_model = biased_transmission_influencer_2(N=100, p_s=0.1, p_low=0.0001, p_0=0.5, t_max=50, r_max=50)\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/"
  },
  {
    "objectID": "chapter08.html#sec-vertical",
    "href": "chapter08.html#sec-vertical",
    "title": "12  Vertical and horizontal transmission",
    "section": "12.1 Vertical cultural transmission",
    "text": "12.1 Vertical cultural transmission\n\nimport numpy as np \nrng = np.random.default_rng()\n\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\ndef vertical_transmission(N, p_0, b, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max): \n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # # For each generation \n        for t in range(1, t_max): \n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n\n            # randomly pick mothers and fathers\n            mother = previous_population[\"trait\"].sample(N, replace=True).reset_index(drop=True)\n            father = previous_population[\"trait\"].sample(N, replace=True).reset_index(drop=True)\n\n            # prepare next generation\n            population = pd.DataFrame({\"trait\": [np.nan] * N })\n\n            # Both parents are A, thus child adopts A\n            both_A = (mother == \"A\") & (father == \"A\")\n            # if sum(both_A) > 0:\n            population.loc[both_A,\"trait\"] = \"A\"\n\n            # Both parents are A, thus child adopts A\n            both_B = (mother == \"B\") & (father == \"B\")\n            # if sum(both_B) > 0:\n            population.loc[both_B,\"trait\"] = \"B\"\n\n            # If any empty NA slots are present (i.e. one A and one B parent) they adopt A with probability b\n            remaining = rng.choice([\"A\", \"B\"], size=population[\"trait\"].isna().sum(), replace=True, p=[b, 1 - b])\n            population.loc[population[\"trait\"].isna(),\"trait\"] = remaining\n            \n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndata_model = vertical_transmission(N=10_000, p_0=0.01, b=0.6,t_max=50, r_max=5)\n\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = vertical_transmission(N=10_000, p_0=0.1, b=0.5,t_max=50, r_max=5)\nplot_multiple_runs(data_model)"
  },
  {
    "objectID": "chapter08.html#horizontal-cultural-transmission",
    "href": "chapter08.html#horizontal-cultural-transmission",
    "title": "12  Vertical and horizontal transmission",
    "section": "12.2 Horizontal cultural transmission",
    "text": "12.2 Horizontal cultural transmission\n\n\n\n\n\n\nWarning\n\n\n\nThe code below is not yet correct and runs very slowly.\n\n\n\ndef vertical_horizontal_transmission(N, p_0, b, n, g, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in tqdm(range(t_max)):\n            ### Vertical transmission =========================================================\n\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n\n            # randomly pick mothers and fathers\n            mother = previous_population[\"trait\"].sample(N, replace=True).reset_index(drop=True)\n            father = previous_population[\"trait\"].sample(N, replace=True).reset_index(drop=True)\n\n            # prepare next generation\n            population = pd.DataFrame({\"trait\": [np.nan] * N })\n\n            # Both parents are A, thus child adopts A\n            both_A = (mother == \"A\") & (father == \"A\")\n            # if sum(both_A) > 0:\n            population.loc[both_A,\"trait\"] = \"A\"\n\n            # Both parents are A, thus child adopts A\n            both_B = (mother == \"B\") & (father == \"B\")\n            # if sum(both_B) > 0:\n            population.loc[both_B,\"trait\"] = \"B\"\n\n            # If any empty NA slots are present (i.e. one A and one B parent) they adopt A with probability b\n            remaining = rng.choice([\"A\", \"B\"], size=population[\"trait\"].isna().sum(), replace=True, p=[b, 1 - b])\n            population.loc[population[\"trait\"].isna(),\"trait\"] = remaining\n            \n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n            ## Horizontal transmission =========================================================\n\n            previous_population = population.copy()\n            # # N_B = number of Bs\n            N_B = previous_population[previous_population[\"trait\"] == \"B\"].shape[0]\n\n            # if there are B individuals to switch, and n is not zero:\n            if (N_B > 0) & (n > 0):\n                # for each B individual:\n                for i in range(N_B):\n                    # Pick n demonstrators\n                    demonstrator = previous_population[\"trait\"].sample(n, replace=True)\n                    # Get probability g \n                    copy_ = rng.choice([True, False], n, p=[g, 1 - g], replace=True)\n                    # if any demonstrators with A are to be copied:\n                    if sum((demonstrator == \"A\") & (copy_)) > 0:\n                      # The B individual switches to A \n                      # TODO: Here's the bug!\n                      population[previous_population[\"trait\"] == \"B\"].at[i, \"trait\"] = \"A\"\n\n            next_population = population.copy()\n            # # N_B = number of Bs\n            N_B = next_population[next_population[\"trait\"] == \"B\"].shape[0]\n\n            # if there are B individuals to switch, and n is not zero:\n            if (N_B > 0) & (n > 0):\n                # for each B individual:\n                for i in range(N_B):\n                    # Pick n demonstrators\n                    demonstrator = population[\"trait\"].sample(n, replace=True)\n                    # Get probability g \n                    copy_ = rng.choice([True, False], n, p=[g, 1 - g], replace=True)\n                    # if any demonstrators with A are to be copied:\n                    if sum((demonstrator == \"A\") & (copy_)) > 0:\n                      # The B individual switches to A \n                      next_population[next_population[\"trait\"] == \"B\"].at[i, \"trait\"] = \"A\"\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = next_population[ next_population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output\n\n\nvertical_horizontal_transmission(N=1000, p_0=0.01, b=0.5, n=5, g=0.1, t_max=10, r_max=1)\n\n\n\n\n\n  \n    \n      \n      generation\n      p\n      run\n    \n  \n  \n    \n      0\n      0\n      0.003\n      0\n    \n    \n      1\n      1\n      0.004\n      0\n    \n    \n      2\n      2\n      0.006\n      0\n    \n    \n      3\n      3\n      0.007\n      0\n    \n    \n      4\n      4\n      0.010\n      0\n    \n    \n      5\n      5\n      0.015\n      0\n    \n    \n      6\n      6\n      0.018\n      0\n    \n    \n      7\n      7\n      0.020\n      0\n    \n    \n      8\n      8\n      0.022\n      0\n    \n    \n      9\n      9\n      0.021\n      0\n    \n  \n\n\n\n\n\n# data_model = vertical_horizontal_transmission(N=5_000, p_0=0.01, b=0.5, n=5, g=0.1, t_max=50, r_max=2)\n# plot_multiple_runs(data_model)\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/"
  },
  {
    "objectID": "chapter09.html#introducing-innovation",
    "href": "chapter09.html#introducing-innovation",
    "title": "13  The multiple traits model",
    "section": "13.1 Introducing innovation",
    "text": "13.1 Introducing innovation"
  },
  {
    "objectID": "pitches.html",
    "href": "pitches.html",
    "title": "15  The usage of pitches and intervals",
    "section": "",
    "text": "“Cross-cultural data shows musical scales evolved to maximise imperfect fifths” McBride & Tlusty (2020)\n“The line of fifths and the co-evolution of tonal pitch-classes” (Moss et al., 2022)\n\n\n\n\n\nMcBride, J. M., & Tlusty, T. (2020). Cross-cultural data shows musical scales evolved to maximise imperfect fifths. http://arxiv.org/abs/1906.06171\n\n\nMoss, F. C., Neuwirth, M., & Rohrmeier, M. (2022). The line of fifths and the co-evolution of tonal pitch-classes. Journal of Mathematics and Music, 1–25. https://doi.org/10.1080/17459737.2022.2044927"
  },
  {
    "objectID": "folk_tunes.html",
    "href": "folk_tunes.html",
    "title": "16  Folk tune complexity",
    "section": "",
    "text": "“The role of population size in folk tune complexity” (Street et al., 2022)\n\n\n\n\nStreet, S., Eerola, T., & Kendal, J. R. (2022). The role of population size in folk tune complexity. Humanities and Social Sciences Communications, 9(1), 1–12. https://doi.org/10.1057/s41599-022-01139-y"
  },
  {
    "objectID": "communities.html",
    "href": "communities.html",
    "title": "17  Music communities",
    "section": "",
    "text": "“Phylogenetic reconstruction of the cultural evolution of electronic music via dynamic community detection (1975–1999)” (Youngblood et al., 2021)\n\n\n\n\nYoungblood, M., Baraghith, K., & Savage, P. E. (2021). Phylogenetic reconstruction of the cultural evolution of electronic music via dynamic community detection (1975–1999). Evolution and Human Behavior. https://doi.org/10.1016/j.evolhumbehav.2021.06.002"
  },
  {
    "objectID": "style_evolution.html",
    "href": "style_evolution.html",
    "title": "18  Style evolution",
    "section": "",
    "text": "“Statistical Evolutionary Laws in Music Styles” (Nakamura & Kaneko, 2019)\n“Investigating style evolution of Western classical music: A computational approach” (Weiß et al., 2019)\n\n\n\n\n\nNakamura, E., & Kaneko, K. (2019). Statistical evolutionary laws in music styles. Nature Scientific Reports, 9(1), 15993. https://doi.org/10.1038/s41598-019-52380-6\n\n\nWeiß, C., Mauch, M., Dixon, S., & Müller, M. (2019). Investigating style evolution of Western classical music: A computational approach. Musicae Scientiae, 23(4), 486–507. https://doi.org/10.1177/1029864918757595"
  },
  {
    "objectID": "conclusion.html#what-can-cultural-evolution-tell-us-about-music",
    "href": "conclusion.html#what-can-cultural-evolution-tell-us-about-music",
    "title": "19  Conclusion",
    "section": "19.1 What can cultural evolution tell us about music",
    "text": "19.1 What can cultural evolution tell us about music"
  },
  {
    "objectID": "conclusion.html#what-is-the-role-of-models-for-musicology",
    "href": "conclusion.html#what-is-the-role-of-models-for-musicology",
    "title": "19  Conclusion",
    "section": "19.2 What is the role of models for musicology",
    "text": "19.2 What is the role of models for musicology"
  },
  {
    "objectID": "conclusion.html#avenues-for-future-research",
    "href": "conclusion.html#avenues-for-future-research",
    "title": "19  Conclusion",
    "section": "19.3 Avenues for future research",
    "text": "19.3 Avenues for future research"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A\nstep-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/\n\n\nAunger, R. (2001). Darwinizing Culture: The\nStatus of Memetics as a Science.\nOxford University Press, USA. http://gen.lib.rus.ec/book/index.php?md5=7329e2aa9adcddfed967088219426193\n\n\nBentley, R. A., Hahn, M. W., & Shennan, S. J. (2004). Random drift\nand culture change. Proceedings of the Royal Society of London.\nSeries B: Biological Sciences, 271(1547), 1443–1450. https://doi.org/10.1098/rspb.2004.2746\n\n\nBishop, C. M. (2012). Model-based machine learning. Philosophical\nTransactions of The Royal Society A, 0222(371). https://doi.org/10.1098/rsta.2012.0222\n\n\nBlackmore, S. (2000). The Meme Machine.\nOxford University Press.\n\n\nBoyd, R., & Richerson, P. J. (1985). Culture and the Evolutionary Process. The\nUniversity of Chicago Press.\n\n\nCavalli-Sforza, L. L., & Feldman, M. W. (1981). Cultural\nTransmission and Evolution.\nPrinceton University Press.\n\n\nCross, I. (2016). The nature of music and its evolution. In S. Hallam,\nI. Cross, & M. Thaut (Eds.), The Oxford Handbook of\nMusic Psychology (2nd ed., pp. 1–20). Oxford\nUniversity Press. https://doi.org/10.1093/oxfordhb/9780199298457.013.0001\n\n\nDawkins, R. (1976). The Selfish Gene. Oxford\nUniversity Press.\n\n\nFarrell, S., & Lewandowsky, S. (2018). Computational\nModeling of Cognition and\nBehavior. Cambridge University Press. https://doi.org/10.1017/CBO9781316272503\n\n\nFinkensiep, C., Neuwirth, M., & Rohrmeier, M. (forthcoming). Music\nTheory and Model-driven Corpus\nResearch. In D. Shanahan, J. A. Burgoyne, & I. Quinn (Eds.),\nOxford Handbook of Music and Corpus\nStudies. Oxford University Press.\n\n\nGjerdingen, R. O. (2007). Music in the Galant\nStyle. Oxford University Press.\n\n\nHoning, H. (2006). Computational Modeling of Music\nCognition: A Case Study on Model\nSelection. Music Perception, 23(5), 365–376. https://doi.org/10.1525/mp.2006.23.5.365\n\n\nHoning, H. (2018). On the biological basis of musicality. Annals of\nthe New York Academy of Sciences. https://doi.org/10.1111/nyas.13638\n\n\nHowe, C. J., & Windram, H. F. (2011). Phylomemetics beyond the\nGene. PLOS Biology, 9(5), e1001069. https://doi.org/10.1371/journal.pbio.1001069\n\n\nJan, S. (2016). The Memetics of Music: A\nNeo-Darwinian View of Musical Structure and Culture.\nRoutledge.\n\n\nMcBride, J. M., & Tlusty, T. (2020). Cross-cultural data shows\nmusical scales evolved to maximise imperfect fifths. http://arxiv.org/abs/1906.06171\n\n\nMcElreath, R. (2020). Statistical Rethinking: A\nBayesian Course with Examples in R and\nSTAN (Second). Chapman and Hall/CRC.\n\n\nMeyer, L. B. (1989). Style and Music.\nTheory, History, and\nIdeology. University of Chicago Press.\n\n\nMorley, I. (2013). The Prehistory of\nMusic. Human Evolution,\nArchaeology, and the Origins of\nMusicality. Oxford University Press.\n\n\nMoss, F. C., Neuwirth, M., & Rohrmeier, M. (2022). The line of\nfifths and the co-evolution of tonal pitch-classes. Journal of\nMathematics and Music, 1–25. https://doi.org/10.1080/17459737.2022.2044927\n\n\nNakamura, E., & Kaneko, K. (2019). Statistical evolutionary laws in\nmusic styles. Nature Scientific Reports, 9(1), 15993.\nhttps://doi.org/10.1038/s41598-019-52380-6\n\n\nPinker, S. (1997). How the mind works. Norton.\n\n\nPiotrowski, M. (2019). Historical Models and Serial\nSources. Journal of European Periodical Studies,\n4(1), 8–18. https://doi.org/10.21825/jeps.v4i1.10226\n\n\nSavage, P. E. (2019). Cultural evolution of music. Palgrave\nCommunications, 5(1), 1–16. https://doi.org/10.1057/s41599-019-0221-1\n\n\nSmaldino, P. E. (2017). Models Are Stupid, and We\nNeed More of Them. In R. R. Vallacher, S. J. Read,\n& A. Nowak (Eds.), Computational Social\nPsychology (First, pp. 311–331). Routledge. https://doi.org/10.4324/9781315173726-14\n\n\nSmaldino, P. E. (2023). Modeling Social Behavior:\nMathematical and Agent-Based Models of\nSocial Dynamics and Cultural Evolution.\nPrinceton University Press.\n\n\nStreet, S., Eerola, T., & Kendal, J. R. (2022). The role of\npopulation size in folk tune complexity. Humanities and Social\nSciences Communications, 9(1), 1–12. https://doi.org/10.1057/s41599-022-01139-y\n\n\nTomlinson, G. (2018). A Million Years of\nMusic. Princeton University Press. https://press.princeton.edu/books/paperback/9781890951528/a-million-years-of-music\n\n\nWallin, N. L., Merker, B., & Brown, S. (Eds.). (2001). The\nOrigins of Music. MIT Press.\n\n\nWeiß, C., Mauch, M., Dixon, S., & Müller, M. (2019). Investigating\nstyle evolution of Western classical music: A\ncomputational approach. Musicae Scientiae, 23(4),\n486–507. https://doi.org/10.1177/1029864918757595\n\n\nYoungblood, M., Baraghith, K., & Savage, P. E. (2021). Phylogenetic\nreconstruction of the cultural evolution of electronic music via dynamic\ncommunity detection (1975–1999). Evolution and Human Behavior.\nhttps://doi.org/10.1016/j.evolhumbehav.2021.06.002\n\n\nYoungblood, M., Ozaki, Y., & Savage, P. E. (forthcoming). Cultural\nevolution and music. In J. Tehrani, J. R. Kendal, & R. L. Kendal\n(Eds.), Oxford Handbook of Cultural\nEvolution. Oxford University Press. https://psyarxiv.com/xsb7v"
  }
]