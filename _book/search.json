[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cultural Evolution and Music",
    "section": "",
    "text": "On these pages you will learn about cultural evolution and music. The overall aim is to attain a basic understanding of formal models in cultural evolution and learn about several recent approaches that apply them to the domain of music.\nWe start with a minimal introduction to the Python programming language that covers the necessary basic skills in order to follow the remainder of the book. Then, we summarize some general ideas about music and cultural evolution.\nSubsequently, we follow an excellent learning path for computational models in cultural evolution by working through the book Individual-based models of cultural evolution (Acerbi, Mesoudi, and Smolla 2022). Chapters X to Y comprise a translation of this resource to the Python programming language that we will use throughout these materials. Finally, we will review and discuss a number of recent publications on music and cultural evolution in the advanced topics section at the end.\n\n\n\n\n\n\nImportant\n\n\n\nNote that the materials here are still under development. Please inform me if you notice any errors or other issues.\n\n\n\n\n\n\n\n\nIf you want to refer to this resource, please cite it as appropriately, e.g.: Moss, F. C. (2022). “Cultural Evolution and Music: Models and Applications in Python”. https://fabianmoss.github.io/cultevopy\n\n\n\n\n\n\n\nAcerbi, Alberto, Alex Mesoudi, and Marco Smolla. 2022. Individual-Based Models of Cultural Evolution: A Step-by-Step Guide Using R. London: Routledge. https://acerbialberto.com/IBM-cultevo/."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "The field of cultural evolution emerged in the 1980’s, and has, in parallel with the advancement of computational facilities, gained momentum. In recent years, several approaches have attempted to apply formal models from cultural evolution to the domain of music (Youngblood, Ozaki, and Savage forthcoming).\nIn the present context, we first introduce some central ideas of cultural evolution and review a few major publications for the domain of music.\n\n\n\n\n\n\n\n\n\nYoungblood, Mason, Yuto Ozaki, and Patrick E. Savage. forthcoming. “Cultural Evolution and Music.” In Oxford Handbook of Cultural Evolution, edited by J. Tehrani, J. R. Kendal, and R. L. Kendal. Oxford University Press. https://psyarxiv.com/xsb7v."
  },
  {
    "objectID": "chapter00.html",
    "href": "chapter00.html",
    "title": "2  A Python primer",
    "section": "",
    "text": "Note\n\n\n\nFrom now on, we will assume that you have a working Python installation running on your computer. You can check this by typing the following into a terminal/console/command line:\npython --version\nIf the version number starts with a 3, you’re all set. If not, please consider one of the many tutorials online on how to install Python."
  },
  {
    "objectID": "chapter00.html#variables-and-types",
    "href": "chapter00.html#variables-and-types",
    "title": "2  A Python primer",
    "section": "2.1 Variables and types",
    "text": "2.1 Variables and types\nVariable assignment in Python is straight-forward. You choose a name for the variable, and assign a value to it using the = operator, for example:\n\nx = 100\n\nassigns the value 100 to the variable x. If we call the variable now, we can see that it has the value we assigned to it:\n\nx\n\n100\n\n\nOf course, we can also assign things other than numbers, for example:\n\nname = \"Fabian\"\n\nWhat we assigned to the variable name is called a string, it has the value \"Fabian\". Strings are sequences of characters.\n\n\n\n\n\n\nTip\n\n\n\nNote that \"Fabian\" is enclosed by double-quotes. Why is this the case? Why could we not just type name = Fabian?\n\n\nWe can also assign a list of things to a variable:\n\nmylist = [1, 2, 3, \"A\", \"B\", \"C\"]\n\nLists are enclosed by square brackets. As you can see, Python allows you to store any kind of data in lists (here: integer numbers and character strings). However, it is good practice to include only—you’ll understand later why.\nAnother structured data type in python are dictionaries. Dictionaries are collections of key-value pairs. For example, a dictionary addresses could store the email addresses of certain people:\n\naddresses = {\n    \"Andrew\" : \"andrew@example.com\",\n    \"Zoe\" : \"zoe@example.com\"\n}\n\nNow, if we wanted to look up Zoe’s email address, we could to so with:\n\naddresses[\"Zoe\"]\n\n'zoe@example.com'"
  },
  {
    "objectID": "chapter00.html#on-repeat",
    "href": "chapter00.html#on-repeat",
    "title": "2  A Python primer",
    "section": "2.2 On repeat",
    "text": "2.2 On repeat\nCoding something is only useful if you can’t do the job as fast or as efficient by yourself. Especially when it comes to repeating the same little thing many, many times, knowing how to code comes in handy.\nAs a simple example, imagine you want to write down all numbers from 1 to 10, or from 1 to 100, or… you get the idea. In Python, you would do it as follows:\n\nfor i in range(10):\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nYou see that this is not exactly what we wanted. We’re seeing numbers from 0 to 9, but we wanted everything from 1 to 10. Before we fix the code to produce the desired result, let’s explain the bits and pieces of the code above. What we just did was to use a so-called for-loop, probably the most common way to repeat things in Python. First we create an iterator variable i (we could have named any other variable name as well), which takes its value from the list of numbers specified by range(10). If only one number n is provided to range(n), it will enumerate all numbers from 0 to n-1. If two arguments are provided, the first one determines the starting number, and the second one stands for the terminating number minus one—confusing, right? So, in order to enumerate all numbers from 1 to 10, we have to write range(1,11). Finally, the print function outputs the value of i for each iteration.\n\nfor i in range(1,11):\n    print(i)\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nVoilà!"
  },
  {
    "objectID": "chapter00.html#functions",
    "href": "chapter00.html#functions",
    "title": "2  A Python primer",
    "section": "2.3 Functions",
    "text": "2.3 Functions\nWith more and more experience in programming, it is likely that your code will become more and more complex. That means that it will become harder to keep track of what every piece of it is supposed to do. A good strategy to deal with this is to aim for writing code that is modular: it can be broken down into smaller units (modules) that are easier to understand. Moreover, it is sometimes necessary to reuse the same code several times. It would, however, be inefficient to write the same lines over and over again. With your code being modular you can wrap the pieces that you need in several places into a function.\nLet’s look at an example! Assume, your (fairly) complex code involves calculating the sum of the products of two numbers. In Python, we use the + operator to calculate sums and the ** operator to raise a number to a certain power (**2 for the square of a number).\n\nx = 3\ny = 5\n\nsum_of_squares = x**2 + y**2\n\nThe variable sum_of_squares now contains the sum of squares of x=3 and y=5. We can inspect the result by calling the variable:\n\nsum_of_squares \n\n34\n\n\nNow, imagine that you would have to do the same calculation several times for different combinations of values for x and y (and always keeping in mind that this stands in for much more complex examples with several lines of code). We can this code in a function:\n\ndef func_sum_of_squares(x, y):\n    return x**2 + y**2\n\nNow, each time we want to calculate a sum of squares, we can do so by simply invoking\n\nfunc_sum_of_squares(5,4)\n\n41\n\n\nAnd, of course, we could chose a shorter name for the function as well:\n\nf = func_sum_of_squares\n\nf(5,4)\n\n41"
  },
  {
    "objectID": "chapter00.html#libraries-youll-love",
    "href": "chapter00.html#libraries-youll-love",
    "title": "2  A Python primer",
    "section": "2.4 Libraries you’ll love",
    "text": "2.4 Libraries you’ll love\nLuckily, you don’t have to programm all functions by yourself. There is a huge community of Python programmers out there that works and collaborates on several libraries. A library is (more or less) simply a collection of certain functions (and some more, but we don’t get into this here). This means, instead of writing a function yourself, you can rely on functions that someone else has programmed.\n\n\n\n\n\n\nDanger\n\n\n\nWhether a Python library or function does actually do what it promises is another story. Popular libraries with tens of thousands of users are very trust-worthy because you can be almost sure that someone would have noticed erroneous behavior. But it is certainly possible that badly-maintained libraries contain errors. So be prudent when using the code of others.\n\n\nOne of the most popular Python libaries is NumPy for numerical computations. We will rely a lot on the functions in this library, especially in order to draw random samples—more on this later! To use the functions or variables from this library, they have to be imported so that you can use them. There are several ways to do this. For example, you can import the libary entirely:\n\nimport numpy\n\nNow, you can use the (approximated) value of \\(\\pi\\) stored in this library by typing\n\nnumpy.pi\n\n3.141592653589793\n\n\nA different way is to just import everything from the library by writing\n\nfrom numpy import * \n\nHere, the * stands for ‘everything’. Now, to use the value of \\(\\pi\\) we could simply type\n\npi\n\n3.141592653589793\n\n\nThis is, however discouraged for the following reason: imagine we had another library, numpy2 that also stores the value of \\(\\pi\\), but less precisely (e.g. only as 3.14). If we write\n\nfrom numpy import * \nfrom numpy2 import * \n\nWe would have imported the variables holding the value of \\(\\pi\\) from both libraries. But, because they have the same name pi. In this case, pi would equal 3.14 because we imported numpy2 last. This is confusing and shouldn’t be so! To avoid this, it is better to keep references to imported libraries explicit. In order not to have to type too much (we’re all lazy, after all), we can define an alias for the library.\n\nimport numpy as np\nnp.pi\n\n3.141592653589793\n\n\nAll functions of NumPy are now accessible with the prefix np.. You can choose any alias when importing a library (it can even by longer than the library name) but certain conventions have emerged that you’re encouraged to follow. Importing the most commonly used Python libraries for data-science tasks (“The data science triad”), use the following:\n\nimport numpy as np # for numerical computations\nimport pandas as pd # for tabular data \nimport matplotlib.pyplot as plt # for data visualization\n\nWe will use all three of them in the following chapters and you’ll learn to love them."
  },
  {
    "objectID": "chapter01.html",
    "href": "chapter01.html",
    "title": "3  Unbiased transmission",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is based on “Chapter 1: Unbiased transmission” in (Acerbi, Mesoudi, and Smolla 2022).\nFirst we import some modules.\nBecause we will model evolutionary processes that are not strictly deterministic, we need to simulate variations due to random change. For this, we can use the default random number generator from the NumPy library and store it in the variable rng.\nNext, we define some basic variables that we take into account for our first model. We consider a population of \\(N=100\\) individuals as well as a time-frame of \\(t_{max}=100\\) generations."
  },
  {
    "objectID": "chapter01.html#simulating-a-population",
    "href": "chapter01.html#simulating-a-population",
    "title": "3  Unbiased transmission",
    "section": "3.1 Simulating a population",
    "text": "3.1 Simulating a population\n\nN = 100\nt_max = 100\n\n\n\n\n\n\n\nNote\n\n\n\nIn general, we use the variable t to designate generation counts.\n\n\nNow we create a variable population that will store the data about our simulated population. This population has either of two traits \"A\" and \"B\", with a certain probability. We store all of this in a so-called ‘data frame’, which is a somewhat fancy, Pandas-specific term for a table.\n\npopulation = pd.DataFrame(\n    {\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True)}\n)\n\nLet’s take this code apart to understand it better. From the Pandas library, which we imported as the alias pd, we create a DataFrame object. The data contained in this the data frame population is specified via a dictionary that has \"trait\" as its key and a fairly complex expression starting with the random number generator rng as its value. What this value says is, from the list [\"A\", \"B\"] choose randomly N instances with replacement (if replace were set to False, we could at most sample 2 values from the list). So, the data frame population should contain 100 randomly sampled values of A’s and B’s. Let’s confirm this:\n\npopulation\n\n\n\n\n\n  \n    \n      \n      trait\n    \n  \n  \n    \n      0\n      B\n    \n    \n      1\n      B\n    \n    \n      2\n      B\n    \n    \n      3\n      A\n    \n    \n      4\n      B\n    \n    \n      ...\n      ...\n    \n    \n      95\n      A\n    \n    \n      96\n      A\n    \n    \n      97\n      B\n    \n    \n      98\n      A\n    \n    \n      99\n      A\n    \n  \n\n100 rows × 1 columns\n\n\n\nAs you can see, population stores a table with 100 rows (many of them omitted here for display reasons) and a single column called ‘trait’. Each row in the ‘trait’ column contains either the value A or B. To the left of the data frame you can see the numbers of rows explicitly spelled out. This is called the data frame’s index.\n\n\n\n\n\n\nNote\n\n\n\nA and B are just placeholder names for any of two mutually exclusive cultural traits. These could be, for example, preference for red over white whine (ignoring people who like rosé as well as people who have no preference). You see already here that this is a massive oversimplification of actual taste preferences. The point here is not to construct a plausible model but rather to gradually build up a simple one in order to understand well its inner workings.\nIt will help to pause for a moment and to think of other examples that “A” and “B” could stand for. Can you come up with a music-related one?\n\n\nWe can count the number of A’s and B’s as follows:\n\npopulation[\"trait\"].value_counts()\n\nA    52\nB    48\nName: trait, dtype: int64\n\n\nYou can read the above code as “From the population table, select the ‘trait’ colum and count its values.”. Since there were only two values to sample from and they were randomly (uniformly) sampled, the number of A’s and the number of B’s should be approximately equal. We can obtain their relative frequencies by adding setting the normalize keyword to True:\n\npopulation[\"trait\"].value_counts(normalize=True)\n\nA    0.52\nB    0.48\nName: trait, dtype: float64"
  },
  {
    "objectID": "chapter01.html#tracing-cultural-change",
    "href": "chapter01.html#tracing-cultural-change",
    "title": "3  Unbiased transmission",
    "section": "3.2 Tracing cultural change",
    "text": "3.2 Tracing cultural change\nWe now create a second data frame output in which we will store the output of our model. This data frame has two columns: generation, which is the number of the simulated generation, and p which stands for “the probability of an individual of having trait A”.\n\noutput = pd.DataFrame(\n    {\n        \"generation\": np.arange(t_max, dtype=int), \n        \"p\": [np.nan] * t_max \n    }\n)\n\nThe generation column contains all numbers from 0 to t_max - 1. Because we count the numbers of generations (rather than assuming a time-continuous process), we specified that numbers in this column have to be intergers (dtype=int). The values for the p column must look cryptic. It literally says: put the np.nan value t_max times into the p colum. np.nan stands for “not a number” (from the NumPy library), since we haven’t assigned any values to this probability yet. 0\n\noutput\n\n\n\n\n\n  \n    \n      \n      generation\n      p\n    \n  \n  \n    \n      0\n      0\n      NaN\n    \n    \n      1\n      1\n      NaN\n    \n    \n      2\n      2\n      NaN\n    \n    \n      3\n      3\n      NaN\n    \n    \n      4\n      4\n      NaN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      95\n      95\n      NaN\n    \n    \n      96\n      96\n      NaN\n    \n    \n      97\n      97\n      NaN\n    \n    \n      98\n      98\n      NaN\n    \n    \n      99\n      99\n      NaN\n    \n  \n\n100 rows × 2 columns\n\n\n\nDon’t worry that both the index and the ‘generation’ column contain all numbers from 0 to 99. We need this later when things become more involved.\nAs the saying goes, from nothing comes nothing, so we have to start somewhere, meaning that we need to assume that the initial probability of having trait A in our population is an actual number. The most sensible thing is to start with the proportions of A and B in our sampled population as a starting value.\nSo, we approximate the probability of an individual having trait A with the relative frequency of trait A in the population:\n\npopulation[\"trait\"].value_counts(normalize=True)[\"A\"]\n\n0.52\n\n\nYou already know this code from above, we just added the [\"A\"] part at the end to select only the relative frequencies of trait A. We want to set this as the value of p of the first generation. This can be achieved with the .loc (location) method:\n\noutput.loc[0, \"p\"] = population[\"trait\"].value_counts(normalize=True)[\"A\"]\n\nIn words, this reads: “Set location 0 (first row) in the p column of the output data frame to the relative frequency of the trait ‘A’ in the population.”"
  },
  {
    "objectID": "chapter01.html#iterating-over-generations",
    "href": "chapter01.html#iterating-over-generations",
    "title": "3  Unbiased transmission",
    "section": "3.3 Iterating over generations",
    "text": "3.3 Iterating over generations\nRecall that we are trying to observe cultural change over the course of t_max = 100 generations. We thus simply repeat what we just did for the first generation: based on the relative frequencies of A’s and B’s in the previous generation, we sample the traits of 100 new individuals for the next generation.\n\nfor t in range(1, t_max):\n    # Copy the population data frame to `previous_population`\n    previous_population = population.copy()\n  \n    # Randomly copy from previous generation's individuals\n    population = previous_population[\"trait\"].sample(N, replace=True).to_frame()\n    \n    # Get p and put it into the output slot for this generation t\n    output.loc[t, \"p\"] = population[\"trait\"].value_counts(normalize=True)[\"A\"]\n\nThis procedure assignes a probability of having trait “A” for each generation (each row of the p colum is filled now):\n\noutput\n\n\n\n\n\n  \n    \n      \n      generation\n      p\n    \n  \n  \n    \n      0\n      0\n      0.52\n    \n    \n      1\n      1\n      0.50\n    \n    \n      2\n      2\n      0.51\n    \n    \n      3\n      3\n      0.46\n    \n    \n      4\n      4\n      0.41\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      95\n      95\n      0.45\n    \n    \n      96\n      96\n      0.41\n    \n    \n      97\n      97\n      0.37\n    \n    \n      98\n      98\n      0.36\n    \n    \n      99\n      99\n      0.36\n    \n  \n\n100 rows × 2 columns\n\n\n\nTo make things easier, we wrap the above code in a function that we’ll call unbiased_transmission that can take different values for the population size N and number of generations t_max as parameters. The code below is exactly the same as above.\n\ndef unbiased_transmission_1(N, t_max):\n    population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True)})\n\n    output = pd.DataFrame({\"generation\": np.arange(t_max, dtype=int), \"p\": [np.nan] * t_max })\n\n    output.loc[0, \"p\"] = population[ population[\"trait\"] == \"A\"].shape[0] / N\n\n    for t in range(1, t_max):\n        # Copy the population tibble to previous_population tibble\n        previous_population = population.copy()\n    \n        # Randomly copy from previous generation's individuals\n        new_population = previous_population[\"trait\"].sample(N, replace=True).to_frame()\n        \n        # Get p and put it into the output slot for this generation t\n        output.loc[t, \"p\"] = new_population[ new_population[\"trait\"] == \"A\"].shape[0] / N\n    \n    return output\n\n\ndata_model = unbiased_transmission_1(N=100, t_max=200)\n\n\ndef plot_single_run(data_model):\n    data_model[\"p\"].plot(ylim=(0,1))\n\n\nplot_single_run(data_model)\n\n\n\n\nSingle run of the unbiased transmission model for a population of \\(N=100\\) individuals and \\(t_{max}=200\\) generations.\n\n\n\n\n\ndata_model = unbiased_transmission_1(N=10_000, t_max=200)\n\n\nplot_single_run(data_model)\n\n\n\n\nSingle run of the unbiased transmission model for a population of \\(N=10,000\\) individuals and \\(t_{max}=200\\) generations.\n\n\n\n\n\ndef unbiased_transmission_2(N, t_max, r_max):\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True)})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n\n            # Randomly compy from previous generation \n            population = population[\"trait\"].sample(N, replace=True).to_frame()\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output\n\n\nunbiased_transmission_2(100, 100, 3)\n\n\n\n\n\n  \n    \n      \n      generation\n      p\n      run\n    \n  \n  \n    \n      0\n      0\n      0.48\n      0\n    \n    \n      1\n      1\n      0.49\n      0\n    \n    \n      2\n      2\n      0.50\n      0\n    \n    \n      3\n      3\n      0.43\n      0\n    \n    \n      4\n      4\n      0.40\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      295\n      95\n      0.76\n      2\n    \n    \n      296\n      96\n      0.80\n      2\n    \n    \n      297\n      97\n      0.85\n      2\n    \n    \n      298\n      98\n      0.87\n      2\n    \n    \n      299\n      99\n      0.81\n      2\n    \n  \n\n300 rows × 3 columns\n\n\n\n\ndata_model = unbiased_transmission_2(N=100, t_max=200, r_max=5)\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\nplot_multiple_runs(data_model)\n\n\n\n\nMultiple runs of the unbiased transmission model for a population of \\(N=100\\) individuals, with average (black line).\n\n\n\n\n\ndata_model = unbiased_transmission_2(N=10_000, t_max=200, r_max=5)\n\n\nplot_multiple_runs(data_model)\n\n\n\n\nMultiple runs of the unbiased transmission model for a population of \\(N=10,000\\) individuals, with average (black line).\n\n\n\n\n\ndef unbiased_transmission_3(N, p_0, t_max, r_max):\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population\n\n            # Randomly compy from previous generation \n            population = population[\"trait\"].sample(N, replace=True).to_frame()\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output\n\n\ndata_model = unbiased_transmission_3(10_000, p_0=.2, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\nAcerbi, Alberto, Alex Mesoudi, and Marco Smolla. 2022. Individual-Based Models of Cultural Evolution: A Step-by-Step Guide Using R. London: Routledge. https://acerbialberto.com/IBM-cultevo/."
  },
  {
    "objectID": "chapter02.html",
    "href": "chapter02.html",
    "title": "4  Unbiased and biased mutation",
    "section": "",
    "text": "def unbiased_mutation(N, mu, p_0, t_max, r_max):\n    # Create an output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n            \n            # Determine \"mutant\" individuals\n            mutate = rng.choice([True, False], size=N, p=[mu, 1-mu], replace=True)\n\n            # TODO: Something is off here! Changing the order of the conditions affects\n            # the result. Should be constant with random noise but converges to either A or B\n\n            # If there are \"mutants\" from A to B \n            conditionA = mutate & (previous_population[\"trait\"] == \"A\")\n            if conditionA.sum() > 0:\n                population.loc[conditionA, \"trait\"] = \"B\"\n\n            # If there are \"mutants\" from B to A\n            conditionB = mutate & (previous_population[\"trait\"] == \"B\")\n            if conditionB.sum() > 0:\n                population.loc[conditionB, \"trait\"] = \"A\"\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\ndata_model = unbiased_mutation(N=100, mu=.05, p_0=0.5, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = unbiased_mutation(N=100, mu=.05, p_0=0.1, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndef biased_mutation(N, mu_b, p_0, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n            \n            # Determine \"mutant\" individuals\n            mutate = rng.choice([True, False], size=N, p=[mu_b, 1-mu_b], replace=True)\n\n            # TODO: Something is off here! Changing the order of the conditions affects\n            # the result. Should be constant with random noise but converges to either A or B\n\n            # If there are \"mutants\" from B to A\n            conditionB = mutate & (previous_population[\"trait\"] == \"B\")\n            if conditionB.sum() > 0:\n                population.loc[conditionB, \"trait\"] = \"A\"\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndata_model = biased_mutation(N = 100, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = biased_mutation(N = 10000, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model <- biased_mutation(N = 10000, mu_b = 0.1, p_0 = 0, t_max = 200, r_max = 5)\nplot_multiple_runs(data_model)"
  },
  {
    "objectID": "chapter03.html",
    "href": "chapter03.html",
    "title": "5  Biased transmission: direct bias",
    "section": "",
    "text": "def plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\ndef biased_transmission_direct(N, s_a, s_b, p_0, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n\n            # For each individual, pick a random individual from the previous generation\n            demonstrator_trait = previous_population[\"trait\"].sample(N, replace=True).reset_index()\n            \n            # Biased probabilities to copy\n            copy_a = rng.choice([True, False], size=N, replace=True, p=[s_a, 1 - s_a])\n            copy_b = rng.choice([True, False], size=N, replace=True, p=[s_b, 1 - s_b])\n\n            # If the demonstrator has trait A and the individual wants to copy A, then copy A\n            condition = copy_a & (demonstrator_trait[\"trait\"] == \"A\")\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"A\"\n\n            # If the demonstrator has trait B and the individual wants to copy B, then copy B\n            condition = copy_b & (demonstrator_trait[\"trait\"] == \"B\")\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"B\"\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndata_model = biased_transmission_direct(N=10_000, s_a=.1, s_b=0, \n                                         p_0=.01, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = biased_transmission_direct(N=10_000, s_a=.6, s_b=.5, \n                                         p_0=.01, t_max=150, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = biased_transmission_direct(N=10_000, s_a=.2, s_b=0, \n                                         p_0=.01, t_max=200, r_max=5)\nplot_multiple_runs(data_model)"
  },
  {
    "objectID": "chapter04.html",
    "href": "chapter04.html",
    "title": "6  Biased transmission: frequency-dependent indirect bias",
    "section": "",
    "text": "def plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\nN = 100\np_0 = .5\nD = 1.\n\n\n# Create first generation\npopulation = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1-p_0])})\n\n\n# Create a DataFrame with a set of 3 randomly-picked demonstrators for each agent\n\ndemonstrators = pd.DataFrame({\n    \"dem1\" : population[\"trait\"].sample(N, replace=True).values,\n    \"dem2\" : population[\"trait\"].sample(N, replace=True).values,\n    \"dem3\" : population[\"trait\"].sample(N, replace=True).values\n})\n\n\n# Visualize the DataFrame\ndemonstrators\n\n\n\n\n\n  \n    \n      \n      dem1\n      dem2\n      dem3\n    \n  \n  \n    \n      0\n      B\n      B\n      A\n    \n    \n      1\n      B\n      A\n      A\n    \n    \n      2\n      B\n      B\n      B\n    \n    \n      3\n      A\n      A\n      A\n    \n    \n      4\n      B\n      A\n      B\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      A\n      B\n      B\n    \n    \n      96\n      A\n      B\n      B\n    \n    \n      97\n      B\n      B\n      B\n    \n    \n      98\n      A\n      A\n      A\n    \n    \n      99\n      B\n      B\n      A\n    \n  \n\n100 rows × 3 columns\n\n\n\n\n# Get the number of A's in each 3-demonstrator combination\nnum_As = (demonstrators == \"A\").apply(sum, axis=1)\nnum_As\n\n0     1\n1     2\n2     0\n3     3\n4     1\n     ..\n95    1\n96    1\n97    0\n98    3\n99    1\nLength: 100, dtype: int64\n\n\n\n# For 3-demonstrator combinations with all A's, set to A\npopulation[ num_As == 3 ] = \"A\"\n# For 3-demonstrator combinations with all B's, set to B\npopulation[ num_As == 0 ] = \"B\"\n\n\nprob_majority = rng.choice([True, False], p=[(2/3 + D/3), 1-(2/3 + D/3)], size=N, replace=True)\nprob_minority = rng.choice([True, False], p=[(1/3 + D/3), 1-(1/3 + D/3)], size=N, replace=True)\n\n\n# 3-demonstrator combinations with two As and one B\ncondition = prob_majority & (num_As == 2)\nif condition.sum() > 0:\n    population.loc[condition, \"trait\"] = \"A\"\ncondition = ~prob_majority & (num_As == 2)\nif condition.sum() > 0:\n    population.loc[condition, \"trait\"] = \"B\"\n\n# 3-demonstrator combinations with two B's and one A\ncondition = ~prob_minority & (num_As == 1)\nif condition.sum() > 0:\n    population.loc[condition, \"trait\"] = \"A\"\ncondition = prob_minority & (num_As == 1)\nif condition.sum() > 0:\n    population.loc[condition, \"trait\"] = \"B\"\n\n\ndemonstrators[\"new_trait\"] = population[\"trait\"]\ndemonstrators\n\n\n\n\n\n  \n    \n      \n      dem1\n      dem2\n      dem3\n      new_trait\n    \n  \n  \n    \n      0\n      B\n      B\n      A\n      A\n    \n    \n      1\n      B\n      A\n      A\n      A\n    \n    \n      2\n      B\n      B\n      B\n      B\n    \n    \n      3\n      A\n      A\n      A\n      A\n    \n    \n      4\n      B\n      A\n      B\n      B\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      A\n      B\n      B\n      A\n    \n    \n      96\n      A\n      B\n      B\n      B\n    \n    \n      97\n      B\n      B\n      B\n      B\n    \n    \n      98\n      A\n      A\n      A\n      A\n    \n    \n      99\n      B\n      B\n      A\n      B\n    \n  \n\n100 rows × 4 columns\n\n\n\n\ndef conformist_transmission(N, p_0, D, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            demonstrators = pd.DataFrame({\n                \"dem1\" : population[\"trait\"].sample(N, replace=True).values,\n                \"dem2\" : population[\"trait\"].sample(N, replace=True).values,\n                \"dem3\" : population[\"trait\"].sample(N, replace=True).values\n            })\n\n            # Get the number of A's in each 3-demonstrator combination\n            num_As = (demonstrators == \"A\").apply(sum, axis=1)\n\n            # For 3-demonstrator combinations with all A's, set to A\n            population[ num_As == 3 ] = \"A\"\n            # For 3-demonstrator combinations with all A's, set to A\n            population[ num_As == 3 ] = \"A\"\n            # For 3-demonstrator combinations with all B's, set to B\n            population[ num_As == 0 ] = \"B\"\n\n            prob_majority = rng.choice([True, False], p=[(2/3 + D/3), 1-(2/3 + D/3)], size=N, replace=True)\n            prob_minority = rng.choice([True, False], p=[(1/3 + D/3), 1-(1/3 + D/3)], size=N, replace=True)\n\n            # 3-demonstrator combinations with two As and one B\n            condition = prob_majority & (num_As == 2)\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"A\"\n            condition = ~prob_majority & (num_As == 2)\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"B\"\n\n            # 3-demonstrator combinations with two B's and one A\n            condition = prob_minority & (num_As == 1)\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"A\"\n            condition = ~prob_minority & (num_As == 1)\n            if condition.sum() > 0:\n                population.loc[condition, \"trait\"] = \"B\"\n            \n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output\n\n\ndata_model = conformist_transmission(N=1_000, p_0 = 0.5, D = 1, t_max = 50, r_max = 10)\nplot_multiple_runs(data_model)"
  },
  {
    "objectID": "chapter05.html",
    "href": "chapter05.html",
    "title": "7  Biased transmission: demonstrator-based indirect bias",
    "section": "",
    "text": "def plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\nN = 100\np_0 = 0.5\np_s = 0.05\n\n\npopulation = pd.DataFrame({\n    \"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1-p_0]),\n    \"status\": rng.choice([\"high\", \"low\"], size=N, replace=True, p=[p_s, 1-p_s])\n})\n\n\npopulation\n\n\n\n\n\n  \n    \n      \n      trait\n      status\n    \n  \n  \n    \n      0\n      A\n      low\n    \n    \n      1\n      A\n      low\n    \n    \n      2\n      B\n      low\n    \n    \n      3\n      B\n      low\n    \n    \n      4\n      B\n      low\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      95\n      B\n      low\n    \n    \n      96\n      A\n      low\n    \n    \n      97\n      A\n      low\n    \n    \n      98\n      B\n      low\n    \n    \n      99\n      A\n      low\n    \n  \n\n100 rows × 2 columns\n\n\n\n\np_low = 0.01\np_demonstrator = np.ones(N)\np_demonstrator[ population[\"status\"] == \"low\" ] = p_low\n\n\nif sum(p_demonstrator) > 0:\n    ps = p_demonstrator / p_demonstrator.sum()\n    demonstrator_index = rng.choice(np.arange(N), size=N, p=ps, replace=True)\n    population[\"trait\"] = population.loc[demonstrator_index, \"trait\"].values\n\n\ndef biased_transmission_demonstrator(N, p_0, p_s, p_low, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n    \n    for r in range(r_max):\n            # Create first generation\n            population = pd.DataFrame({\n                \"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1-p_0]),\n                \"status\": rng.choice([\"high\", \"low\"], size=N, replace=True, p=[p_s, 1-p_s])\n            })\n            \n            # Assign copying probabilities based on individuals' status\n            p_demonstrator = np.ones(N)\n            p_demonstrator[population[\"status\"] == \"low\"] = p_low\n            \n            # Add first generation's p for run r\n            output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n            \n            for t in range(1, t_max):\n                # Copy individuals to previous_population DataFrame\n                previous_population = population.copy()\n                \n                # Copy traits based on status\n                if sum(p_demonstrator) > 0:\n                    ps = p_demonstrator / p_demonstrator.sum()\n                    demonstrator_index = rng.choice(np.arange(N), size=N, p=ps, replace=True)\n                    population[\"trait\"] = population.loc[demonstrator_index, \"trait\"].values\n                \n                # Get p and put it into output slot for this generation t and run r\n                output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n                \n    return output\n\n\ndata_model = biased_transmission_demonstrator(N=100, p_s=0.05, p_low=0.0001, p_0=0.5, t_max=50, r_max=10)\n\n\nplot_multiple_runs(data_model)\n\n\n\n\n\ndata_model = biased_transmission_demonstrator(N=10_000, p_s=0.005, p_low=0.0001, p_0=0.5, t_max=200, r_max=10)\nplot_multiple_runs(data_model)\n\n\n\n\n\ndef biased_transmission_demonstrator_2(N, p_0, p_s, p_low, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n    \n    ...\n    \n    return output\n\n\ndata_model = biased_transmission_demonstrator_2(N=100, p_s=0.1, p_low=0.0001, p_0=0.5, t_max=50, r_max=50)"
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "8  Advanced topics",
    "section": "",
    "text": "“Cultural Evolution of Music” (Savage 2019)\n“The role of population size in folk tune complexity” (Street, Eerola, and Kendal 2022)\n“Statistical Evolutionary Laws in Music Styles” (Nakamura and Kaneko 2019)\n“The line of fifths and the co-evolution of tonal pitch-classes” (Moss, Neuwirth, and Rohrmeier 2022)\n“” (Youngblood, Ozaki, and Savage forthcoming)\n\n\n\n\n\nMoss, Fabian C., Markus Neuwirth, and Martin Rohrmeier. 2022. “The Line of Fifths and the Co-Evolution of Tonal Pitch-Classes.” Journal of Mathematics and Music, 1–25. https://doi.org/10.1080/17459737.2022.2044927.\n\n\nNakamura, Eita, and Kunihiko Kaneko. 2019. “Statistical Evolutionary Laws in Music Styles.” Nature Scientific Reports 9 (1): 15993. https://doi.org/10.1038/s41598-019-52380-6.\n\n\nSavage, Patrick E. 2019. “Cultural Evolution of Music.” Palgrave Communications 5 (1): 1–16. https://doi.org/10.1057/s41599-019-0221-1.\n\n\nStreet, Sally, Tuomas Eerola, and Jeremy R. Kendal. 2022. “The Role of Population Size in Folk Tune Complexity.” Humanities and Social Sciences Communications 9 (1): 1–12. https://doi.org/10.1057/s41599-022-01139-y.\n\n\nYoungblood, Mason, Yuto Ozaki, and Patrick E. Savage. forthcoming. “Cultural Evolution and Music.” In Oxford Handbook of Cultural Evolution, edited by J. Tehrani, J. R. Kendal, and R. L. Kendal. Oxford University Press. https://psyarxiv.com/xsb7v."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acerbi, Alberto, Alex Mesoudi, and Marco Smolla. 2022.\nIndividual-Based Models of Cultural Evolution: A Step-by-Step Guide\nUsing R. London: Routledge. https://acerbialberto.com/IBM-cultevo/.\n\n\nMoss, Fabian C., Markus Neuwirth, and Martin Rohrmeier. 2022. “The\nLine of Fifths and the Co-Evolution of Tonal Pitch-Classes.”\nJournal of Mathematics and Music, 1–25. https://doi.org/10.1080/17459737.2022.2044927.\n\n\nNakamura, Eita, and Kunihiko Kaneko. 2019. “Statistical\nEvolutionary Laws in Music Styles.” Nature Scientific\nReports 9 (1): 15993. https://doi.org/10.1038/s41598-019-52380-6.\n\n\nSavage, Patrick E. 2019. “Cultural Evolution of Music.”\nPalgrave Communications 5 (1): 1–16. https://doi.org/10.1057/s41599-019-0221-1.\n\n\nStreet, Sally, Tuomas Eerola, and Jeremy R. Kendal. 2022. “The\nRole of Population Size in Folk Tune Complexity.” Humanities\nand Social Sciences Communications 9 (1): 1–12. https://doi.org/10.1057/s41599-022-01139-y.\n\n\nYoungblood, Mason, Yuto Ozaki, and Patrick E. Savage. forthcoming.\n“Cultural Evolution and Music.” In Oxford Handbook of\nCultural Evolution, edited by J. Tehrani, J. R. Kendal, and R. L.\nKendal. Oxford University Press. https://psyarxiv.com/xsb7v."
  }
]